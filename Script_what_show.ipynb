{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75649c68-15bf-4f4c-addc-f29c411ffb0b",
   "metadata": {},
   "source": [
    "# Set up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda4608-e9fd-4156-aa99-ec795004bfc0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fca7f-8ed7-464d-81d4-8064718c69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SCIPY_ARRAY_API\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b46916-da7b-47a7-9e55-025a30a05aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OS and Operations\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Vizualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import CamembertTokenizer, CamembertForTokenClassification, pipeline\n",
    "\n",
    "# Statistic analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import (\n",
    "    spearmanr,\n",
    "    chi2_contingency,\n",
    "    pearsonr,\n",
    "    kendalltau,\n",
    "    mannwhitneyu,\n",
    "    entropy\n",
    ")\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (\n",
    "    mutual_info_score,\n",
    "    normalized_mutual_info_score,\n",
    "    adjusted_mutual_info_score,\n",
    ")\n",
    "\n",
    "# NLP\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import fuzz, process\n",
    "import stanza\n",
    "import fitz\n",
    "    \n",
    "# LLMs\n",
    "import openai\n",
    "import anthropic\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "\n",
    "# Display\n",
    "from IPython.display import display, clear_output\n",
    "from moviepy import ImageSequenceClip\n",
    "import warnings\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5b36d-8f35-4ecb-b6af-b60629192356",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d06291-613f-45e9-a218-e1213c2a728a",
   "metadata": {},
   "source": [
    "## Paths, Data, Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba203f7b-8ac8-4fe9-ab91-6e977a290575",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH=\"/Users/antonioslagarias/Documents/OFF/Exports/adho\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05baf57f-ace8-42da-8366-3af6960872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_PATH=\"/Users/antonioslagarias/Documents/OFF/Exports/adho/figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7602e-0414-44d0-a8b4-5daff37a2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_string(gpt_output):\n",
    "    if not isinstance(gpt_output, str):\n",
    "        return None\n",
    "\n",
    "    # Extract content between first '{' and last '}'\n",
    "    match = re.search(r\"\\{.*\\}\", gpt_output, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0).strip()\n",
    "    return None  # Return None if no valid JSON found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5bf51-4a03-41fa-b69c-7b0deacdcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', errors='ignore').decode('utf-8')\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c23ea0-da73-4169-b0bf-ccf484395956",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/Users/antonioslagarias/Documents/OFF/Exports/data_2013/festival_data_draft_2013.xlsx\"  \n",
    "data_draft=pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab396e-44e6-4ab5-83a8-a0e1741a4988",
   "metadata": {},
   "source": [
    "data_draft.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac1f62-643a-453c-a6e1-38edd8f48387",
   "metadata": {},
   "source": [
    "# Import Data after Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a21d8-5791-457e-a7a8-fc7350f61ef2",
   "metadata": {},
   "source": [
    "See separate notebook for the code used for scrapping from Officiel des Spectacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01bb3a-530c-45a5-a5b1-bddbc6ed2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/Users/antonioslagarias/Documents/OFF/Exports/adho/offi_enriched_2013_nona.csv\"  \n",
    "data_offi=pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6eb81-2afd-4457-b5d4-e8f54ed6bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_offi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ee637-c748-428d-a9c2-c2ead205d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/Users/antonioslagarias/Documents/OFF/Exports/adho/data_adho_.12072025.xlsx\"  \n",
    "data_draft=pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9b091-de28-4e39-a2b8-7e6c6300c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6843247-851c-4450-aa6b-003d3b7d1530",
   "metadata": {},
   "source": [
    "## Prepare Scrapped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4155c8a-9dc4-4468-b648-d3faab8ecdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22983644-128d-4506-97d1-9ee8b17b132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_offi['enriched'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24284a5-1ae2-4cc4-bbef-c2c7d70df596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rows in data_draft: {len(data_draft)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d8c47-7229-48aa-a083-a1c5e29c52f3",
   "metadata": {},
   "source": [
    "## Get Matches with Scrapped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647920f-2db2-4a52-a74a-61505d412728",
   "metadata": {},
   "source": [
    "Matches are based on a fuzzy match logic. For the same creator, a much in a shows titles with a score over 55 percent is considered valid. Outliers (e.g. shows programmed 8 or more years after 2013 are mannually verified below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43a2b8-cdb8-4ba6-a644-90e392356774",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_offi['enriched'] = data_offi['enriched'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c70da7-7be3-4438-bec7-bb326e3299b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entries_with_years(enriched):\n",
    "    if not isinstance(enriched, dict):\n",
    "        return []\n",
    "    results = []\n",
    "    for year, entries in enriched.items():\n",
    "        for entry in entries:\n",
    "            title = entry.get('title', '').strip()\n",
    "            if title:\n",
    "                results.append((entry, year))  # Keep full entry\n",
    "    return results\n",
    "\n",
    "all_matches_per_row = []\n",
    "\n",
    "for _, row in data_draft.iterrows():\n",
    "    id_val = str(row['ID'])  # safe if already string\n",
    "    target_title = row['title']\n",
    "\n",
    "    enriched_row = data_offi[data_offi['ID'] == id_val]\n",
    "    if enriched_row.empty:\n",
    "        all_matches_per_row.append(None)\n",
    "        continue\n",
    "\n",
    "    enriched = enriched_row.iloc[0]['enriched']\n",
    "    entry_year_pairs = get_entries_with_years(enriched)\n",
    "\n",
    "    if not entry_year_pairs:\n",
    "        all_matches_per_row.append(None)\n",
    "        continue\n",
    "\n",
    "    titles = [entry.get('title', '') for entry, _ in entry_year_pairs]\n",
    "\n",
    "    matches = process.extract(target_title, titles, scorer=fuzz.token_sort_ratio, limit=None)\n",
    " \n",
    "    filtered_matches = []\n",
    "    for matched_title, score, idx in matches:\n",
    "        if score >= 55:\n",
    "            entry, year = entry_year_pairs[idx]\n",
    "\n",
    "            # Prepare dict with all available enriched info, prefixed with 'offi_'\n",
    "            match_dict = {\n",
    "                'match_score': score,\n",
    "                'matched_title': matched_title,\n",
    "                'offi_year': year\n",
    "            }\n",
    "\n",
    "            for k, v in entry.items():\n",
    "                match_dict[f'offi_{k}'] = v\n",
    "\n",
    "            filtered_matches.append(match_dict)\n",
    "\n",
    "    all_matches_per_row.append(filtered_matches if filtered_matches else None)\n",
    "\n",
    "data_draft['all_matches'] = all_matches_per_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7838bce-3160-4c47-a929-2f09a7430fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(text):\n",
    "    \"\"\"Lowercase, remove accents, punctuation, and collapse whitespace.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = unidecode(text.lower())\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)      # Normalize spaces\n",
    "    return text.strip()\n",
    "\n",
    "def get_entries_with_years(enriched):\n",
    "    if not isinstance(enriched, dict):\n",
    "        return []\n",
    "    results = []\n",
    "    for year, entries in enriched.items():\n",
    "        for entry in entries:\n",
    "            title = entry.get('title', '').strip()\n",
    "            if title:\n",
    "                results.append((entry, year))\n",
    "    return results\n",
    "\n",
    "all_matches_per_row = []\n",
    "\n",
    "for _, row in data_draft.iterrows():\n",
    "    id_val = str(row['ID'])\n",
    "    target_title = row['title']\n",
    "    target_norm = normalize_title(target_title)\n",
    "\n",
    "    enriched_row = data_offi[data_offi['ID'] == id_val]\n",
    "    if enriched_row.empty:\n",
    "        all_matches_per_row.append(None)\n",
    "        continue\n",
    "\n",
    "    enriched = enriched_row.iloc[0]['enriched']\n",
    "    entry_year_pairs = get_entries_with_years(enriched)\n",
    "\n",
    "    if not entry_year_pairs:\n",
    "        all_matches_per_row.append(None)\n",
    "        continue\n",
    "\n",
    "    raw_titles = [entry.get('title', '') for entry, _ in entry_year_pairs]\n",
    "    norm_titles = [normalize_title(t) for t in raw_titles]\n",
    "\n",
    "    matches = process.extract(target_norm, norm_titles, scorer=fuzz.token_sort_ratio, limit=None)\n",
    "\n",
    "    filtered_matches = []\n",
    "    for matched_norm, score, idx in matches:\n",
    "        if score >= 55:\n",
    "            entry, year = entry_year_pairs[idx]\n",
    "            match_dict = {\n",
    "                'match_score': score,\n",
    "                'matched_title': raw_titles[idx],\n",
    "                'offi_year': year\n",
    "            }\n",
    "            for k, v in entry.items():\n",
    "                match_dict[f'offi_{k}'] = v\n",
    "            filtered_matches.append(match_dict)\n",
    "\n",
    "    all_matches_per_row.append(filtered_matches if filtered_matches else None)\n",
    "\n",
    "data_draft['all_matches'] = all_matches_per_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17545524-27d1-4cca-a201-6962bfae89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft['all_matches'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640cb65c-58d3-4aa8-a31f-0f7306c037f0",
   "metadata": {},
   "source": [
    "Create a new dataframe to examine shows reprogrammed in multiple years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb7567-2824-42e6-ac11-18507ae64e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Explode the all_matches column\n",
    "df_exploded = data_draft.explode('all_matches', ignore_index=True)\n",
    "\n",
    "# Step 2: Drop rows with no match\n",
    "df_exploded = df_exploded[df_exploded['all_matches'].notna()].copy()\n",
    "\n",
    "# Step 3: Normalize the dictionary in each match into columns\n",
    "match_details = pd.json_normalize(df_exploded['all_matches'])\n",
    "\n",
    "# Step 4: Combine into full exploded DataFrame\n",
    "df_exploded = df_exploded.drop(columns=['all_matches']).reset_index(drop=True)\n",
    "df_exploded = pd.concat([df_exploded, match_details], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedfda3-c114-4afa-99e9-46eeef440d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded['API_names_matched'] = np.nan\n",
    "df_exploded['API_names_found'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a64bd-fe4a-4ffd-91f5-4b0311eb070b",
   "metadata": {},
   "source": [
    "Further filtering the matches. Now matches are accepted only when at least one name of a collaborators are identified within the already matched shows based on the creator and the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7260e5-6610-4266-b3ce-3efcabddd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names_from_api_roles(api_roles):\n",
    "    if pd.isna(api_roles):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if isinstance(api_roles, str):\n",
    "            roles_dict = json.loads(api_roles)\n",
    "        elif isinstance(api_roles, dict):\n",
    "            roles_dict = api_roles\n",
    "        else:\n",
    "            return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    names = []\n",
    "    for role_list in roles_dict.values():\n",
    "        if isinstance(role_list, list):\n",
    "            names.extend(role_list)\n",
    "    return [normalize_text(name) for name in names if isinstance(name, str)]\n",
    "\n",
    "# Apply matching\n",
    "matched_names = []\n",
    "has_match_flags = []\n",
    "\n",
    "for _, row in df_exploded.iterrows():\n",
    "    offi_roles = normalize_text(row.get('offi_roles', ''))\n",
    "    api_names = extract_names_from_api_roles(row.get('API_roles', {}))\n",
    "\n",
    "    if offi_roles:\n",
    "        matched = [name for name in api_names if name in offi_roles]\n",
    "    else:\n",
    "        matched = []  # ❗ ensure matched is always defined\n",
    "\n",
    "    matched_names.append(matched if matched else None)\n",
    "    has_match_flags.append(bool(matched))\n",
    "\n",
    "# Add results to DataFrame\n",
    "df_exploded['API_names_matched'] = matched_names\n",
    "df_exploded['API_names_found'] = has_match_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5507c6-ce22-4e1d-afe7-3dc61574b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = df_exploded['API_names_found'].sum()\n",
    "print(f\"Number of rows with at least one matched API name: {true_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c26a53-e19d-416b-90d0-57edb052bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_with_match_count = df_exploded[df_exploded['API_names_found']].ID.nunique()\n",
    "print(f\"Number of unique IDs with at least one API name match: {id_with_match_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdaefd-e039-4ba6-9eab-c06af822ebf8",
   "metadata": {},
   "source": [
    "Manual verification for shows that have no casting information or appear as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961b61e-b0e0-4395-9eef-4a831965e464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exploded_filtered = df_exploded[\n",
    "    df_exploded['match_score'].between(55, 80, inclusive='left') &\n",
    "    (df_exploded['API_names_found'] == True)\n",
    "].copy()\n",
    "df_exploded_filtered.to_excel(\"shows_to_verify.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a8a169-b147-4f0e-95b0-63579bbff7fa",
   "metadata": {},
   "source": [
    "Set further limits after manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cac47-291e-4e52-9b16-f9caf97fdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure year is integer\n",
    "df_exploded['offi_year'] = df_exploded['offi_year'].astype(int)\n",
    "\n",
    "# Apply filtering rules\n",
    "df_exploded = df_exploded[\n",
    "    (\n",
    "        ((df_exploded['offi_year'].between(2010, 2017)) & (df_exploded['match_score'] > 57))\n",
    "    )\n",
    "    |\n",
    "    ((df_exploded['offi_year'] > 2017) & (df_exploded['match_score'] > 75))\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b721d2-10fd-4929-ab22-ebcd47252b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fad3c-a4d2-48ca-9341-a68e776e63a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize offi_roles to detect empty entries\n",
    "df_exploded['offi_roles_normalized'] = df_exploded['offi_roles'].apply(normalize_text)\n",
    "df_exploded['offi_title_normalized'] = df_exploded['offi_title'].apply(normalize_text)\n",
    "\n",
    "# Determine where roles is empty but name matched in title\n",
    "step4_matches = []\n",
    "\n",
    "for _, row in df_exploded.iterrows():\n",
    "    if row['offi_roles_normalized']:  # Skip if roles is non-empty\n",
    "        step4_matches.append(False)\n",
    "        continue\n",
    "\n",
    "    api_names = extract_names_from_api_roles(row.get('API_roles', {}))\n",
    "    title_text = row['offi_title_normalized']\n",
    "\n",
    "    matched = any(name in title_text for name in api_names)\n",
    "    step4_matches.append(matched)\n",
    "\n",
    "# Add a column flag to identify Step 4 logic\n",
    "df_exploded['matched_from_title'] = step4_matches\n",
    "\n",
    "# Filter only those that matched via Step 4\n",
    "df_step4_only = df_exploded[df_exploded['matched_from_title']]\n",
    "\n",
    "# Show selected columns for inspection\n",
    "df_step4_only[['ID', 'title', 'offi_title', 'API_roles', 'API_names_matched', 'offi_roles','API_names_found', \"offi_year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15230151-e15a-43f9-ac5a-12065b0a3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step4_only = df_exploded[df_exploded['matched_from_title']].copy()\n",
    "df_step4_only['step4_keep'] = False  # default, to be manually changed\n",
    "indexes_to_keep = [59, 313, 314, 315, 332, 348, 349, 350, 364, 461, 473, 595, 664, 665, 738, 769, 888, 889, 892, 930]\n",
    "df_step4_only.loc[indexes_to_keep, 'step4_keep'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab9fb6-92c6-417c-bddf-fbcdd3aa7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df_exploded.merge(\n",
    "    df_step4_only[['step4_keep']],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with False\n",
    "df_exploded['step4_keep'] = df_exploded['step4_keep'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61089d3-df94-4d97-b845-e5859cec3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec98e8-5a9b-40e8-8ebe-d2fd49bde580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded=df_exploded.drop(columns=['offi_roles_normalized',\n",
    "       'offi_title_normalized', 'matched_from_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a0dba-d8bc-4851-aeeb-1cb789449dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded['reprog'] = df_exploded['API_names_found'] | df_exploded['step4_keep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa958a7-d2d0-4ae4-ba46-9ffc5566d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = df_exploded['reprog'].sum()\n",
    "print(f\"Number of rows with at least one matched API name: {true_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff26f3-13bf-4fc8-8612-f82a88d3a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_with_match_count = df_exploded[df_exploded['reprog']].ID.nunique()\n",
    "print(f\"Number of unique IDs with at least one reprog match: {id_with_match_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df1a0f-8aea-40ff-9999-81077b1ba748",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_with_reprog = df_exploded.loc[df_exploded['reprog'], 'ID'].unique()\n",
    "data_draft['reprog'] = data_draft['ID'].isin(ids_with_reprog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ef1d0-2cf9-4b1f-b7f9-00aa38af2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_with_match_count = data_draft[data_draft['reprog']].ID.nunique()\n",
    "print(f\"Number of unique IDs with at least one reprog match: {id_with_match_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b03406-1acc-46c0-b2da-d00753f7491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_unique = data_draft['ID'].nunique()\n",
    "print(f\"Number of unique IDs: {id_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666357a-43de-4217-88ea-e26a23a8ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "per=id_with_match_count/id_unique*100\n",
    "print(f\"{round(per, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b6c3f-886f-4dee-bdb8-0d1b48a37329",
   "metadata": {},
   "source": [
    "# Descriptives and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653aaaad-e699-4fe9-8c24-b7db75f25c56",
   "metadata": {},
   "source": [
    "## Global Descriptives on Repogramming Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d1a0c-5d12-485b-ab67-ae36288957a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reprog = df_exploded[df_exploded['reprog']].copy()\n",
    "\n",
    "# Normalize year to int\n",
    "df_reprog['offi_year'] = df_reprog['offi_year'].astype(int)\n",
    "\n",
    "# Aggregate per ID\n",
    "reprog_summary = (\n",
    "    df_reprog\n",
    "    .groupby('ID')\n",
    "    .agg(\n",
    "        first_year=('offi_year', 'min'),\n",
    "        all_years=('offi_year', lambda x: sorted(set(x))),\n",
    "        count_years=('offi_year', 'nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Optional: mark shows with multiple years\n",
    "reprog_summary['multiple_years'] = reprog_summary['count_years'] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a3da3-8959-4c66-80ad-c71fbb5f1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    reprog_summary,\n",
    "    x=\"first_year\",\n",
    "    nbins=len(reprog_summary['first_year'].unique()),\n",
    "    title=\"Number of Reprogrammed Shows by First Year\",\n",
    "    labels={\"first_year\": \"First Reprog Year\"},\n",
    ")\n",
    "fig.update_layout(xaxis=dict(tickmode='linear'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb38e1-d070-4e53-8c81-8fe8862a07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every show (ID) work out its first reprog year\n",
    "first_year_per_id = (\n",
    "    df_reprog.groupby('ID')['offi_year']\n",
    "    .min()\n",
    "    .rename('first_year')\n",
    ")\n",
    "\n",
    "df_reprog = df_reprog.merge(first_year_per_id, on='ID', how='left')\n",
    "\n",
    "# Tag each row as “first” vs “subsequent”\n",
    "df_reprog['is_first'] = df_reprog['offi_year'] == df_reprog['first_year']\n",
    "\n",
    "# Aggregate counts by calendar year & category\n",
    "year_counts = (\n",
    "    df_reprog\n",
    "    .groupby(['offi_year', 'is_first'])\n",
    "    .size()\n",
    "    .reset_index(name='n')\n",
    ")\n",
    "\n",
    "# Plot: one bar per year, coloured by first / subsequent\n",
    "fig = px.bar(\n",
    "    year_counts,\n",
    "    x='offi_year',\n",
    "    y='n',\n",
    "    color='is_first',\n",
    "    color_discrete_map={True: 'steelblue', False: 'orange'},\n",
    "    barmode='group',\n",
    "    labels=dict(offi_year='Year', n='Number of Shows', is_first='Category'),\n",
    "    title='Reprogrammed Shows per Year (first vs. subsequent)'\n",
    ")\n",
    "fig.update_layout(xaxis=dict(tickmode='linear'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ca4b8-96bb-485d-beec-4c18499c6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual verification for outliers\n",
    "ids_after_2018 = df_reprog.loc[df_reprog['offi_year'] > 2018, 'ID'].unique()\n",
    "\n",
    "# Extract all rows (all years) for those shows\n",
    "to_verify = df_reprog[df_reprog['ID'].isin(ids_after_2018)].copy()\n",
    "\n",
    "# Optional: sort by ID and year for easier review\n",
    "to_verify = to_verify.sort_values(['ID', 'offi_year'])\n",
    "\n",
    "# Export to Excel\n",
    "to_verify.to_excel(\"shows_with_year_after_2018_2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75d0c5-c071-47c9-b335-2e156f745f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ids            = data_draft['ID'].nunique()      # every show\n",
    "total_ids_reprog     = df_exploded['ID'].nunique()     # shows that have ≥1 reprog\n",
    "\n",
    "first_year_df = (\n",
    "    df_reprog               # df_reprog already = df_exploded[df_exploded['reprog']]\n",
    "    .groupby('ID')['offi_year']\n",
    "    .min()\n",
    "    .reset_index(name='first_year')\n",
    ")\n",
    "\n",
    "first_year_counts = (\n",
    "    first_year_df\n",
    "    .groupby('first_year')['ID']\n",
    "    .nunique()\n",
    "    .reset_index(name='first_year_count')\n",
    "    .rename(columns={'first_year': 'year'})\n",
    ")\n",
    "\n",
    "\n",
    "first_year_counts['percent_all']          = (first_year_counts['first_year_count'] / total_ids        * 100).round(2)\n",
    "first_year_counts['percent_reprog_only']  = (first_year_counts['first_year_count'] / total_ids_reprog * 100).round(2)\n",
    "\n",
    "print(\"First-year reprogramming rates:\")\n",
    "print(first_year_counts[['year', 'first_year_count', 'percent_all', 'percent_reprog_only']]\n",
    "      .to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aec0ab-ece8-483f-b34f-6b050453e52e",
   "metadata": {},
   "source": [
    "### Seperate pre- and post-festival shows for ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495073e-ab58-480f-ba41-98561d7c28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# French month names (lower-case for matching)\n",
    "french_months = [\n",
    "    \"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\",\n",
    "    \"juillet\", \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\"\n",
    "]\n",
    "\n",
    "# Build a regex that matches any of the above as whole words\n",
    "month_pattern = r'\\b(?:' + '|'.join(french_months) + r')\\b'\n",
    "\n",
    "# Create a new column `offi_months`\n",
    "#     • For 2013 rows: list of matched months (lower-case strings)\n",
    "#     • For other years: empty list\n",
    "def extract_months(row):\n",
    "    if row['offi_year'] != 2013 or not isinstance(row['offi_dates'], str):\n",
    "        return []                       \n",
    "    return re.findall(month_pattern, row['offi_dates'].lower())\n",
    "\n",
    "df_exploded['offi_months'] = df_exploded.apply(extract_months, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b12cfd-911c-4d1f-aaa5-c3800f19b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of French months\n",
    "month_order = {\n",
    "    \"janvier\": 1, \"février\": 2, \"mars\": 3, \"avril\": 4, \"mai\": 5, \"juin\": 6,\n",
    "    \"juillet\": 7, \"août\": 8, \"septembre\": 9, \"octobre\": 10, \"novembre\": 11, \"décembre\": 12\n",
    "}\n",
    "\n",
    "# Function to determine if first month is before July\n",
    "def is_pre_festival(months):\n",
    "    if not months:\n",
    "        return False\n",
    "    first_month = months[0]\n",
    "    return month_order.get(first_month, 13) < 7\n",
    "\n",
    "# Apply to create new column\n",
    "df_exploded['pro-festival'] = df_exploded['offi_months'].apply(is_pre_festival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8b731-826f-4b2a-9067-fab25ea6f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 2013 only\n",
    "df_2013 = df_exploded[df_exploded['offi_year'] == 2013]\n",
    "\n",
    "# Count True/False values in 'pro-festival'\n",
    "festival_counts_2013 = df_2013['pro-festival'].value_counts()\n",
    "\n",
    "# Display results\n",
    "print(\"Pro-festival counts for 2013:\")\n",
    "print(festival_counts_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e42f92-0c8a-466d-a602-2f708b48583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and pro-festival flag\n",
    "festival_bar_data = (\n",
    "    df_exploded[df_exploded['offi_year'] == 2013]\n",
    "    .groupby(['offi_year', 'pro-festival'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d800c-3b95-4c8c-abb0-0d795a507001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. data prep ────────────────────────────────────────────────────\n",
    "df = df_exploded[df_exploded['reprog']].copy()\n",
    "\n",
    "df['kind'] = np.where(\n",
    "    df['offi_year'] == df.groupby('ID')['offi_year'].transform('min'),\n",
    "    'First Reprog.', 'Subseq. Reprog.'\n",
    ")\n",
    "\n",
    "df['grp'] = np.where(\n",
    "    df['offi_year'] == 2013,\n",
    "    np.where(df['pro-festival'], 'Pro', 'NonPro'),\n",
    "    'All'\n",
    ")\n",
    "\n",
    "# ── 2. aggregate counts ─────────────────────────────────────────────\n",
    "agg = (df.groupby(['offi_year', 'grp', 'kind'])['ID']\n",
    "         .nunique()\n",
    "         .reset_index(name='n'))\n",
    "\n",
    "# ── 3. position bars on the x-axis ──────────────────────────────────\n",
    "def xpos(r):\n",
    "    if r.offi_year == 2013 and r.grp != 'All':\n",
    "        return r.offi_year - 0.3 if r.grp == 'Pro' else r.offi_year + 0.3\n",
    "    return r.offi_year\n",
    "\n",
    "agg['x'] = agg.apply(xpos, axis=1)\n",
    "\n",
    "# ── 4. build the figure ─────────────────────────────────────────────\n",
    "col = {'First Reprog.': 'steelblue', 'Subseq. Reprog.': 'darkorange'}\n",
    "fig = go.Figure()\n",
    "\n",
    "for kind in ['First Reprog.', 'Subseq. Reprog.']:\n",
    "    d = agg[agg['kind'] == kind]\n",
    "    fig.add_bar(\n",
    "        x=d['x'],\n",
    "        y=d['n'],\n",
    "        name=kind,\n",
    "        legendgroup=kind,\n",
    "        offsetgroup=kind,\n",
    "        marker_color=col[kind]\n",
    "    )\n",
    "\n",
    "# Vertical separator at July 2013\n",
    "fig.add_shape(\n",
    "    type='line',\n",
    "    x0=2013, x1=2013,\n",
    "    y0=0, y1=agg['n'].max() * 1.15,\n",
    "    line=dict(color='black', width=3)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='Reprogrammed Shows – split for 2013',\n",
    "    xaxis=dict(\n",
    "        title='Year',\n",
    "        tickmode='linear',\n",
    "        linecolor='black',\n",
    "        mirror=False,\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='N. of Shows',\n",
    "        linecolor='black',\n",
    "        mirror=False,  # mirror removed here\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    legend=dict(\n",
    "    title='',\n",
    "    borderwidth=0,\n",
    "    font=dict(size=10),\n",
    "    x=0.84,\n",
    "    y=1,\n",
    "    xanchor='left',\n",
    "    yanchor='top'\n",
    ")\n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "png_path = os.path.join(FIG_PATH, \"reprog_bar_split.png\")\n",
    "pdf_path = os.path.join(FIG_PATH, \"reprog_bar_split.pdf\")\n",
    "fig.write_image(png_path, scale=3, width=900, height=600)\n",
    "fig.write_image(pdf_path, width=900, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85b83c-359f-4a12-9d72-8e627d33fda8",
   "metadata": {},
   "source": [
    "### Estimate the average life of a show "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42c068-22dd-49a8-a96a-f5b1edb7d26e",
   "metadata": {},
   "source": [
    "This estimation is made based on the number fuzzy matches per show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e55ed-b000-460b-95c8-c027df466a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows with reprog == True\n",
    "df_reprog = df_exploded[df_exploded['reprog']].copy()\n",
    "\n",
    "# Compute life as number of rows per ID\n",
    "life_counts = (\n",
    "    df_reprog.groupby('ID')\n",
    "    .size()\n",
    "    .rename('life')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Compute median\n",
    "median_life = life_counts['life'].median()\n",
    "\n",
    "# Plot histogram\n",
    "fig_life = px.histogram(\n",
    "    life_counts,\n",
    "    x='life',\n",
    "    histfunc='count',\n",
    "    title='Distribution of Show Lifespans (Row Count)',\n",
    "    labels={'life': 'Lifespan (number of rows)'},\n",
    "    color_discrete_sequence=['steelblue']\n",
    ")\n",
    "\n",
    "fig_life.update_layout(\n",
    "    yaxis_title='Number of Shows',\n",
    "    xaxis=dict(dtick=1),\n",
    "    bargap=0.1\n",
    ")\n",
    "\n",
    "# Add median line\n",
    "fig_life.add_vline(\n",
    "    x=median_life,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"crimson\",\n",
    "    annotation_text=f\"Median: {median_life:.1f}\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "\n",
    "fig_life.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ade0f-aaf2-4e55-80c4-d4b14c186a38",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Keep only rows with reprog == True\n",
    "df_reprog = df_exploded[df_exploded['reprog']].copy()\n",
    "\n",
    "# Compute life as number of rows per ID\n",
    "life_counts = (\n",
    "    df_reprog.groupby('ID')\n",
    "    .size()\n",
    "    .rename('life')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Median lifespan\n",
    "median_life = life_counts['life'].median()\n",
    "\n",
    "# Percentage with life < 3\n",
    "percent_under_3 = 100 * (life_counts['life'] < 3).sum() / len(life_counts)\n",
    "percent_under_4 = 100 * (life_counts['life'] < 4).sum() / len(life_counts)\n",
    "\n",
    "# Print results\n",
    "print(f\"Median lifespan: {median_life:.2f} years\")\n",
    "print(f\"Percentage of shows with lifespan < 3 years: {percent_under_3:.2f}%\")\n",
    "print(f\"Percentage of shows with lifespan < 4 years: {percent_under_4:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a331dd-a484-4e7c-80af-751013c9ddb1",
   "metadata": {},
   "source": [
    "Export Dataframe for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2469462-356a-4c03-a516-c93377a6f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817983d4-4b37-436a-85a9-b9b2aa6ba791",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=\"12072025\"\n",
    "file = f\"/Users/antonioslagarias/Documents/OFF/Exports/adho/data_adho_.{date}.xlsx\"\n",
    "data_draft.to_excel(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c2347-a9b8-4496-ab23-f7dd3648eebf",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8324d4-f730-452c-9d4d-a99b1e41f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/Users/antonioslagarias/Documents/OFF/Exports/adho/data_adho_prog.xlsx\"  \n",
    "data_draft=pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214ef7d-56a8-4449-9466-08746641e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67049a7-bc45-4803-a36e-8406715ee5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab16ba7-16df-4af9-8a4c-8697d5aa2683",
   "metadata": {},
   "source": [
    "#### Preparing the data, remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64ea0f-7fe9-4fe0-b264-f2800f211666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to count names in \"Interprètes\" / \"Interprète\" / \"avec\" categories\n",
    "def count_interpretes(cell):\n",
    "    if pd.isna(cell):\n",
    "        return 0\n",
    "    # try JSON → literal_eval fallback\n",
    "    try:\n",
    "        data = json.loads(cell)\n",
    "    except Exception:\n",
    "        try:\n",
    "            data = ast.literal_eval(cell)\n",
    "        except Exception:\n",
    "            return 0\n",
    "    count = 0\n",
    "    for key, value in data.items():\n",
    "        key_norm = key.lower()\n",
    "        if any(x in key_norm for x in (\"interprète\", \"interprete\", \"interprètes\", \"interpretes\", \"avec\")):\n",
    "            if isinstance(value, list):\n",
    "                count += len(value)\n",
    "            elif isinstance(value, str):\n",
    "                # split simple name lists\n",
    "                names = [n.strip() for n in re.split(r',|;| et ', value) if n.strip()]\n",
    "                count += len(names)\n",
    "    return count\n",
    "\n",
    "# add new column with the count\n",
    "data_draft[\"n_actors\"] = data_draft[\"API_roles\"].apply(count_interpretes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fae0c-56a2-4f03-bbac-72b95ab1acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data_draft.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4084292-56c2-4190-ac01-af04d09c36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration_minutes\"] = pd.to_numeric(df[\"duration_minutes\"], errors=\"coerce\")\n",
    "df[\"duration_minutes\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a977a1-18b0-4101-a125-3e34aa2cadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df[\"duration_minutes\"].isin([0, 1440, 3300])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7874d1f-e006-4f86-8de0-ff62a04a1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_actors\"] = pd.to_numeric(df[\"n_actors\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306560b7-ef6c-4140-ade8-58d5c7d49e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_actors\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a0aa4-9bb4-42eb-9746-055cdd9f1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reprog\"] = df[\"reprog\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e675dc-1bdc-4c97-9e25-c8838d91711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(table):\n",
    "    \"\"\"Cramér’s V.\"\"\"\n",
    "    chi2, _, _, _ = chi2_contingency(table)\n",
    "    n = table.values.sum()\n",
    "    r, k = table.shape\n",
    "    return np.sqrt((chi2 / n) / (min(k - 1, r - 1)))\n",
    "\n",
    "def cramers_v_bergsma(table):\n",
    "    \"\"\"Bergsma-corrected Cramér’s V\"\"\"\n",
    "    chi2, _, _, _ = chi2_contingency(table)\n",
    "    n = table.values.sum()\n",
    "    r, k = table.shape\n",
    "    phi2 = chi2 / n\n",
    "    phi2_corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    r_corr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    k_corr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    return np.sqrt(phi2_corr / min(k_corr - 1, r_corr - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb5b2a-232e-4902-8f3e-5e76902d90c4",
   "metadata": {},
   "source": [
    "#### Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10c435-d791-4b5c-ab32-6e1bb3c8dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actors\n",
    "actors = (\n",
    "    df.groupby(\"n_actors\")[\"reprog\"]\n",
    "      .agg(total=\"count\", reprog_1=\"sum\")\n",
    "      .assign(percent=lambda d: 100 * d[\"reprog_1\"] / d[\"total\"])\n",
    "      .sort_values(\"percent\", ascending=False)\n",
    ")\n",
    "\n",
    "actors[\"percent\"] = actors[\"percent\"].round(1)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  # optional: see all rows\n",
    "print(\"🏛️ Percentage of shows programmed (reprog = 1) by number of actors:\\n\")\n",
    "display(actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc67e2-c139-4644-8e5a-ef9ecdf6afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"n_actors\"] == 23, \"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166ed56-275b-4158-9c58-c7dd8c5f00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[[\"n_actors\", \"reprog\"]].dropna()\n",
    "df_corr = df_corr[df_corr[\"n_actors\"] > 0]\n",
    "\n",
    "# Pearson correlation (equivalent to point-biserial when one var is binary)\n",
    "r, p = stats.pearsonr(df_corr[\"n_actors\"], df_corr[\"reprog\"])\n",
    "\n",
    "# Display\n",
    "print(f\"Filtered rows used: {len(df_corr)}\")\n",
    "print(f\"Point-biserial correlation r  = {r:.3f}\")\n",
    "print(f\"p-value                        = {p:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4debb3-e583-4aed-9704-9b0b3a6d8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[[\"n_actors\", \"reprog\"]].dropna()\n",
    "df_corr = df_corr[df_corr[\"n_actors\"] > 0]\n",
    "\n",
    "# Define feature and target\n",
    "X = df_corr[[\"n_actors\"]]  # must be 2D\n",
    "y = df_corr[\"reprog\"]\n",
    "\n",
    "# Fit simple logistic model using only this feature\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict probabilities\n",
    "y_scores = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(y, y_scores)\n",
    "\n",
    "# Display\n",
    "print(f\"Filtered rows used: {len(df_corr)}\")\n",
    "print(f\"AUC (1-feature model): {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866a7df-3b43-48a5-b1b9-4d7ea428101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"n_actors\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49f8b6-9db5-4e4e-ba1c-98b95511f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"n_actors\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926dbd1-8d6c-449c-96ef-2ee827efbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_actors\"].hist(bins=20)\n",
    "plt.xlabel(\"Number of actors\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of n_actors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063ffcb-7727-4b82-962d-eea4a8555df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_val = df[\"n_actors\"].value_counts(normalize=True).iloc[0]\n",
    "print(f\"Top frequency proportion: {top_val:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b956ccc-0aa7-4366-adf4-8d29e5362fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "group0 = df[df[\"reprog\"] == 0][\"n_actors\"]\n",
    "group1 = df[df[\"reprog\"] == 1][\"n_actors\"]\n",
    "u_stat, p = mannwhitneyu(group0, group1)\n",
    "print(f\"U Stat = {u_stat:.3f},  p = {p:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386581a-4343-41ae-a32d-198110b19dd8",
   "metadata": {},
   "source": [
    "Spline predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195573c-7ce2-4f9a-8884-817c75dd4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spline-transformed predictor\n",
    "\n",
    "# 1. Filter rows where n_actors > 0 and not null\n",
    "df_filtered = df[df[\"n_actors\"].notna() & (df[\"n_actors\"] > 0)].copy()\n",
    "\n",
    "# 2. Generate the spline basis\n",
    "spline = dmatrix(\n",
    "    \"bs(n_actors, df=4, degree=3, include_intercept=False)\",\n",
    "    data=df_filtered,\n",
    "    return_type='dataframe'\n",
    ")\n",
    "\n",
    "X = sm.add_constant(spline)\n",
    "model_act = sm.Logit(df_filtered[\"reprog\"], X).fit()\n",
    "print(model_act.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adc4a7-cc84-46fb-9c65-677203d54141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_act.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070f535-4de3-44b8-9079-5b9f738ff218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create range of values\n",
    "x_vals = np.linspace(df[\"n_actors\"].min(), df[\"n_actors\"].max(), 100)\n",
    "spline_basis = dmatrix(\"bs(x_vals, df=4, degree=3, include_intercept=False)\", {\"x_vals\": x_vals}, return_type='dataframe')\n",
    "\n",
    "# Predict probabilities\n",
    "X_pred = sm.add_constant(spline_basis)\n",
    "y_pred = model_act.predict(X_pred)\n",
    "\n",
    "plt.plot(x_vals, y_pred)\n",
    "plt.xlabel(\"n_actors\")\n",
    "plt.ylabel(\"Predicted probability of reprog\")\n",
    "plt.title(\"Nonlinear effect of n_actors on reprog\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474104e-7f05-4143-97a6-6a354f3f1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels\n",
    "df[\"actor_group\"] = pd.Series(pd.NA, index=df.index)\n",
    "\n",
    "bins = [0, 1, 3, 5, 9, np.inf]\n",
    "labels = [\"0–1\", \"2–3\", \"4–5\", \"6–9\", \"10+\"]\n",
    "\n",
    "bins = [0, 1, 3, 5, 9, np.inf]\n",
    "labels = [\"1\", \"2–3\", \"4–5\", \"6–9\", \"10+\"]\n",
    "df[\"actor_group\"] = pd.cut(df[\"n_actors\"], bins=bins, labels=labels, right=True)\n",
    "\n",
    "df[\"actor_group\"] = pd.cut(df[\"n_actors\"], bins=bins, labels=labels)\n",
    "\n",
    "# Check group sizes\n",
    "print(df[\"actor_group\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4073450-c3d4-4669-bad4-86aa1f83d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = df.groupby(\"actor_group\")[\"reprog\"].mean().reset_index()\n",
    "\n",
    "sns.barplot(x=\"actor_group\", y=\"reprog\", data=group_means)\n",
    "plt.ylabel(\"Reprogramming rate\")\n",
    "plt.title(\"Reprog rate by actor group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d8f4f-4780-4703-bc86-12e58de329ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean reprog rate per group\n",
    "group_means = df.groupby(\"actor_group\")[\"reprog\"].mean().reset_index()\n",
    "\n",
    "# Format and print as percentages\n",
    "for _, row in group_means.iterrows():\n",
    "    group = row[\"actor_group\"]\n",
    "    rate = row[\"reprog\"] * 100  # Convert to percentage\n",
    "    print(f\"Actor group {group}: {rate:.1f}% reprogramming rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dfbc0-fc70-45b1-aaad-213e10b93b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: One-hot encode (and convert to float to avoid dtype=object)\n",
    "X = pd.get_dummies(df[\"actor_group\"].dropna(), drop_first=True).astype(float)\n",
    "\n",
    "# Step 3: Add constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 4: Define y and match indices (in case any NA rows were dropped)\n",
    "y = df.loc[X.index, \"reprog\"]\n",
    "\n",
    "# Step 5: Fit the model\n",
    "model = sm.Logit(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6071624-371b-4b22-9a55-bd9668357671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(\"actor_group\")[\"reprog\"].value_counts().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597412fd-7bf8-47ae-b84d-1a9abebd72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Odds ratios:\")\n",
    "print(np.exp(model.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa67aa7-568f-4db0-9021-8983d5614033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. Filter NaN actor_group entries once ─────────────────────────────\n",
    "df_valid = df[df[\"actor_group\"].notna()].copy()\n",
    "\n",
    "# ── 1. Observed statistics ─────────────────────────────────────────────\n",
    "table_obs = pd.crosstab(df_valid[\"actor_group\"], df_valid[\"reprog\"])\n",
    "chi2, p_chi2, dof, expected = chi2_contingency(table_obs)\n",
    "\n",
    "v_obs        = cramers_v(table_obs)\n",
    "v_corr_obs   = cramers_v_bergsma(table_obs)\n",
    "\n",
    "# ── 2. Bootstrap confidence intervals ──────────────────────────────────\n",
    "B = 5_000\n",
    "boot_v        = np.empty(B)\n",
    "boot_v_corr   = np.empty(B)\n",
    "\n",
    "for i in range(B):\n",
    "    sample_df = df_valid.sample(n=len(df_valid), replace=True)\n",
    "    tbl = pd.crosstab(sample_df[\"actor_group\"], sample_df[\"reprog\"])\n",
    "    boot_v[i]      = cramers_v(tbl)\n",
    "    boot_v_corr[i] = cramers_v_bergsma(tbl)\n",
    "\n",
    "ci_v        = np.percentile(boot_v,        [2.5, 97.5])\n",
    "ci_v_corr   = np.percentile(boot_v_corr,   [2.5, 97.5])\n",
    "\n",
    "# ── 3. Permutation tests ───────────────────────────────────────────────\n",
    "P = 10_000\n",
    "perm_v        = np.empty(P)\n",
    "perm_v_corr   = np.empty(P)\n",
    "\n",
    "reprog_vals = df_valid[\"reprog\"].values.copy()\n",
    "for i in range(P):\n",
    "    np.random.shuffle(reprog_vals)\n",
    "    tbl = pd.crosstab(df_valid[\"actor_group\"], reprog_vals)\n",
    "    perm_v[i]      = cramers_v(tbl)\n",
    "    perm_v_corr[i] = cramers_v_bergsma(tbl)\n",
    "\n",
    "p_perm_v      = (np.sum(perm_v      >= v_obs      ) + 1) / (P + 1)\n",
    "p_perm_vcorr  = (np.sum(perm_v_corr >= v_corr_obs) + 1) / (P + 1)\n",
    "\n",
    "# ── 4. Print results ───────────────────────────────────────────────────\n",
    "print(f\"Chi-square statistic      : {chi2:.3f}  (dof={dof}, p={p_chi2:.3f})\")\n",
    "\n",
    "print(f\"Cramér’s V : {v_obs:.3f}  \"\n",
    "      f\"[95 % CI {ci_v[0]:.3f} – {ci_v[1]:.3f}]  \"\n",
    "      f\"Permutation p = {p_perm_v:.3f}\")\n",
    "\n",
    "print(f\"Cramér’s V (Bergsma corr.): {v_corr_obs:.3f}  \"\n",
    "      f\"[95 % CI {ci_v_corr[0]:.3f} – {ci_v_corr[1]:.3f}]  \"\n",
    "      f\"Permutation p = {p_perm_vcorr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec4900-cd0f-4633-b1b1-72f05ed3d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_valid[\"actor_group\"].astype(\"category\").cat.codes\n",
    "Y = df_valid[\"reprog\"].astype(int)\n",
    "\n",
    "# Mutual information I(X; Y)\n",
    "mi = mutual_info_score(X, Y)\n",
    "\n",
    "# Entropy H(Y)\n",
    "p_y = np.bincount(Y) / len(Y)\n",
    "h_y = entropy(p_y, base=2)\n",
    "\n",
    "# Theil's U\n",
    "theils_u = mi / h_y if h_y != 0 else 0\n",
    "\n",
    "print(f\"Theil’s U (reprog | actors): {theils_u:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d9bba-d7a1-478b-8ecc-f2e753448b56",
   "metadata": {},
   "source": [
    "#### Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54e63a-f8da-4dad-887c-c9fd4a3c12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"duration_minutes\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214d516-0609-4331-83c4-a0666d451ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration_minutes\"].hist(bins=20)\n",
    "plt.xlabel(\"Duration\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e63106-1711-476f-95e3-57f7d3c08854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers and shows by duration \n",
    "duration_clean = df[(df[\"duration_minutes\"] >= 30) & (df[\"duration_minutes\"] <= 105)].copy()\n",
    "\n",
    "bins = [29, 45, 60, 70, 85, 110]\n",
    "labels = [\"30–44\", \"45–59\", \"60–69\", \"70–84\", \"85–105\"]\n",
    "\n",
    "duration_clean[\"duration_group\"] = pd.cut(duration_clean[\"duration_minutes\"], bins=bins, labels=labels)\n",
    "\n",
    "print(duration_clean[\"duration_group\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0b9b5-c620-40e6-8a16-c0ef49cf04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprog_order = sorted(duration_clean[\"reprog\"].unique())\n",
    "\n",
    "# Compute median duration for reprog = 0 and reprog = 1\n",
    "medians = duration_clean.groupby(\"reprog\")[\"duration_minutes\"].median()\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.stripplot(\n",
    "    data=duration_clean,\n",
    "    x=\"reprog\",\n",
    "    y=\"duration_minutes\",\n",
    "    palette=[\"#1f77b4\", \"#ff7f0e\"],  # blue for 0, orange for 1\n",
    "    jitter=True,\n",
    "    order=reprog_order,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add red horizontal median lines\n",
    "for i, rep_val in enumerate(reprog_order):\n",
    "    plt.hlines(\n",
    "        y=medians[rep_val],\n",
    "        xmin=i - 0.2, xmax=i + 0.2,\n",
    "        color=\"red\", linewidth=2\n",
    "    )\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Style\n",
    "plt.title(\"Distribution by Reprogramming\")\n",
    "plt.xlabel(\"Reprog. in Paris\")\n",
    "plt.ylabel(\"Duration (min.)\")\n",
    "plt.xticks([0, 1], [\"No\", \"Yes\"])\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to PDF (optional)\n",
    "file = os.path.join(FIG_PATH, \"duration_vs_reprog_bins.pdf\")\n",
    "plt.savefig(file, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Saved plot to {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affc91a-7d73-4646-b3e7-7c34cabc48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed statistics\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "table_obs = pd.crosstab(duration_clean[\"duration_group\"], duration_clean[\"reprog\"])\n",
    "chi2, p_chi2, dof, _ = chi2_contingency(table_obs)\n",
    "\n",
    "v_obs       = cramers_v(table_obs)\n",
    "v_corr_obs  = cramers_v_bergsma(table_obs)\n",
    "\n",
    "\n",
    "# Bootstrap 95 % CIs\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "B = 5_000\n",
    "boot_v, boot_v_corr = np.empty(B), np.empty(B)\n",
    "\n",
    "for i in range(B):\n",
    "    samp = duration_clean.sample(len(duration_clean), replace=True)\n",
    "    tbl  = pd.crosstab(samp[\"duration_group\"], samp[\"reprog\"])\n",
    "    boot_v[i]      = cramers_v(tbl)\n",
    "    boot_v_corr[i] = cramers_v_bergsma(tbl)\n",
    "\n",
    "ci_v       = np.percentile(boot_v,       [2.5, 97.5])\n",
    "ci_v_corr  = np.percentile(boot_v_corr,  [2.5, 97.5])\n",
    "\n",
    "\n",
    "# Permutation test (P = 10 000)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "P = 10_000\n",
    "perm_v, perm_v_corr = np.empty(P), np.empty(P)\n",
    "reprog_vals = duration_clean[\"reprog\"].values.copy()\n",
    "\n",
    "for i in range(P):\n",
    "    np.random.shuffle(reprog_vals)\n",
    "    tbl = pd.crosstab(duration_clean[\"duration_group\"], reprog_vals)\n",
    "    perm_v[i]      = cramers_v(tbl)\n",
    "    perm_v_corr[i] = cramers_v_bergsma(tbl)\n",
    "\n",
    "p_perm_v      = (np.sum(perm_v      >= v_obs     ) + 1) / (P + 1)\n",
    "p_perm_vcorr  = (np.sum(perm_v_corr >= v_corr_obs) + 1) / (P + 1)\n",
    "\n",
    "# Print\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(f\"Chi-square statistic      : {chi2:.3f}  (dof={dof}, p={p_chi2:.3f})\")\n",
    "print(f\"Cramér’s V  : {v_obs:.3f}  \"\n",
    "      f\"[95 % CI {ci_v[0]:.3f} – {ci_v[1]:.3f}]  \"\n",
    "      f\"Permutation p = {p_perm_v:.5f}\")\n",
    "print(f\"Cramér’s V (Bergsma corr.): {v_corr_obs:.3f}  \"\n",
    "      f\"[95 % CI {ci_v_corr[0]:.3f} – {ci_v_corr[1]:.3f}]  \"\n",
    "      f\"Permutation p = {p_perm_vcorr:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869f155-6990-4933-a1c4-81565bb0fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = duration_clean[\"duration_group\"].astype(\"category\").cat.codes\n",
    "Y = duration_clean[\"reprog\"].astype(\"int\")\n",
    "\n",
    "# Compute mutual information I(X; Y)\n",
    "mi = mutual_info_score(X, Y)\n",
    "\n",
    "# Compute entropy H(Y)\n",
    "p_y = np.bincount(Y) / len(Y)\n",
    "h_y = entropy(p_y, base=2)\n",
    "\n",
    "# Compute Theil's U (Y | X)\n",
    "theils_u = mi / h_y if h_y != 0 else 0\n",
    "\n",
    "print(f\"Theil’s U (reprog | duration groups): {theils_u:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e3de1-fed6-4d1d-875e-3cc46710f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc = duration_clean[[\"duration_group\", \"reprog\"]].dropna()\n",
    "\n",
    "# 2. Compute mean reprog rate per group\n",
    "duration_probs = df_auc.groupby(\"duration_group\")[\"reprog\"].mean()\n",
    "\n",
    "# 3. Map back to each row\n",
    "df_auc[\"duration_score\"] = df_auc[\"duration_group\"].map(duration_probs)\n",
    "\n",
    "# 4. Compute AUC\n",
    "auc_value = roc_auc_score(df_auc[\"reprog\"], df_auc[\"duration_score\"])\n",
    "\n",
    "print(f\"AUC (duration_group): {auc_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2573f-5550-4783-9392-4eba66e873c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation\n",
    "r, p = stats.pearsonr(duration_clean[\"duration_minutes\"], duration_clean[\"reprog\"].dropna())\n",
    "\n",
    "# Display\n",
    "print(f\"Filtered rows used: {len(df_corr)}\")\n",
    "print(f\"Point-biserial correlation r  = {r:.3f}\")\n",
    "print(f\"p-value                        = {p:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e28fe-853d-4ed8-a3af-59993c078433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = duration_clean[[\"duration_minutes\", \"reprog\"]].dropna()\n",
    "\n",
    "# Define feature and target\n",
    "X = df_corr[[\"duration_minutes\"]]  # must be 2D\n",
    "y = df_corr[\"reprog\"]\n",
    "\n",
    "# Fit simple logistic model using only this feature\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict probabilities\n",
    "y_scores = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(y, y_scores)\n",
    "\n",
    "# Display\n",
    "print(f\"Filtered rows used: {len(df_corr)}\")\n",
    "print(f\"AUC (1-feature model): {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20da124-6924-4f7d-9dcb-e90e3abbfdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the spline basis\n",
    "# 4 degrees of freedom (= 4 basis splines)\n",
    "spline_basis = dmatrix(\n",
    "    \"bs(duration_minutes, df=4, degree=3, include_intercept=False)\",\n",
    "    data=duration_clean,\n",
    "    return_type=\"dataframe\"\n",
    ")\n",
    "\n",
    "X = sm.add_constant(spline_basis)     # add intercept\n",
    "y = duration_clean[\"reprog\"]\n",
    "\n",
    "\n",
    "model_duration= sm.Logit(y, X).fit()\n",
    "print(model_duration.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbe597-bc04-4ef7-a58d-ab8f7f81385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_duration.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb03dc-a79e-44f0-ba2d-d5bcde8842e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "duration_grid = np.linspace(duration_clean[\"duration_minutes\"].min(),\n",
    "                            duration_clean[\"duration_minutes\"].max(), 250)\n",
    "\n",
    "basis_grid = dmatrix(\n",
    "    \"bs(duration_minutes, df=4, degree=3, include_intercept=False)\",\n",
    "    {\"duration_minutes\": duration_grid},\n",
    "    return_type=\"dataframe\"\n",
    ")\n",
    "\n",
    "X_grid = sm.add_constant(basis_grid)\n",
    "\n",
    "# Convert to NumPy for correct matrix operations\n",
    "X_mat = X_grid.to_numpy()\n",
    "cov_mat = model_duration.cov_params().to_numpy()\n",
    "\n",
    "# Prediction and 95% CI\n",
    "pred_prob = model_duration.predict(X_grid)\n",
    "logit_pred = model_duration.predict(X_grid, which=\"linear\")\n",
    "\n",
    "se = np.sqrt(np.sum((X_mat @ cov_mat) * X_mat, axis=1))\n",
    "logit_ci_low = logit_pred - 1.96 * se\n",
    "logit_ci_high = logit_pred + 1.96 * se\n",
    "prob_ci_low = 1 / (1 + np.exp(-logit_ci_low))\n",
    "prob_ci_high = 1 / (1 + np.exp(-logit_ci_high))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Main line and CI in orange tones\n",
    "plt.plot(duration_grid, pred_prob, lw=2, color=\"#e6550d\", label=\"Predicted probability\")\n",
    "plt.fill_between(duration_grid, prob_ci_low, prob_ci_high,\n",
    "                 alpha=0.3, color=\"#fdae6b\", label=\"95 % CI\")\n",
    "\n",
    "plt.xlabel(\"Duration (minutes)\", color=\"black\")\n",
    "plt.ylabel(\"P(reprog = 1)\", color=\"black\")\n",
    "\n",
    "plt.grid(False)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['bottom'].set_color(\"black\")\n",
    "ax.spines['left'].set_color(\"black\")\n",
    "ax.spines['bottom'].set_linewidth(1.2)\n",
    "ax.spines['left'].set_linewidth(1.2)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.title(\"Spline-based Logistic Fit: Duration → Reprog\", color=\"black\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b2e13-1726-466f-8cd5-101fbe595dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfx = model_duration.get_margeff(at=\"overall\", method=\"dydx\")\n",
    "print(\"\\n📉 Marginal Effects:\")\n",
    "print(mfx.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fa865-9097-4706-bebe-191cbf42828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot with histogram under curve\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Primary axis: predicted probability curve\n",
    "ax1.plot(duration_grid, pred_prob, label=\"Pred. probability\", color=\"#e6550d\")\n",
    "ax1.fill_between(duration_grid, prob_ci_low, prob_ci_high, alpha=0.3, color=\"#fdae6b\", label=\"95% CI\")\n",
    "ax1.set_xlabel(\"Duration (min.)\", color=\"black\")\n",
    "ax1.set_ylabel(\"P(reprog = 1)\", color=\"black\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"black\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_color(\"black\")\n",
    "ax1.spines['left'].set_color(\"black\")\n",
    "ax1.spines['bottom'].set_linewidth(1.2)\n",
    "ax1.spines['left'].set_linewidth(1.2)\n",
    "ax1.grid(False)\n",
    "\n",
    "# Secondary axis: histogram\n",
    "ax2 = ax1.twinx()\n",
    "counts, bins, _ = ax2.hist(duration_clean[\"duration_minutes\"], bins=30, color=\"gray\", alpha=0.3)\n",
    "ax2.set_ylabel(\"Show count (histogram)\", color=\"gray\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"gray\")\n",
    "ax2.set_ylim(0, max(counts) * 1.2)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "\n",
    "# Final touches\n",
    "fig.suptitle(\"Predicted Probability of Reprogramming by Show Duration\\nwith Histogram of Observations\", color=\"black\")\n",
    "fig.tight_layout()\n",
    "ax1.legend(loc=\"upper left\", frameon=False)\n",
    "png_path = os.path.join(FIG_PATH, \"duration_line.png\")\n",
    "pdf_path = os.path.join(FIG_PATH, \"duration_line.pdf\")\n",
    "plt.savefig(png_path, dpi=600, bbox_inches=\"tight\", transparent=False)\n",
    "plt.savefig(pdf_path, dpi=600, bbox_inches=\"tight\", transparent=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063d5b3-d965-40d5-b9ed-d9ead6a068c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1️⃣ Histogram: duration distribution split by reprog ──────────────\n",
    "plt.figure(figsize=(7, 5))\n",
    "bins = np.arange(30, 106, 5)  # 5-minute bins\n",
    "\n",
    "plt.hist(duration_clean[duration_clean[\"reprog\"] == 0][\"duration_minutes\"],\n",
    "         bins=bins, alpha=0.6, label=\"Not Reprogrammed\", color=\"#3182bd\")\n",
    "plt.hist(duration_clean[duration_clean[\"reprog\"] == 1][\"duration_minutes\"],\n",
    "         bins=bins, alpha=0.6, label=\"Reprogrammed\", color=\"#fdae6b\")\n",
    "\n",
    "plt.xlabel(\"Duration (minutes)\", color=\"black\")\n",
    "plt.ylabel(\"Number of Shows\", color=\"black\")\n",
    "\n",
    "for s in [\"bottom\", \"left\"]:\n",
    "    plt.gca().spines[s].set_color(\"black\")\n",
    "    plt.gca().spines[s].set_linewidth(1.2)\n",
    "for s in [\"top\", \"right\"]:\n",
    "    plt.gca().spines[s].set_visible(False)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.title(\"Distribution of Show Durations by Reprogramming Status\", color=\"black\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ── 2️⃣ Fitted spline curve with CI (orange style) ──────────────────\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(duration_grid, pred_prob, lw=2, color=\"#e6550d\", label=\"Predicted probability\")\n",
    "plt.fill_between(duration_grid, prob_ci_low, prob_ci_high, alpha=0.3, color=\"#fdae6b\", label=\"95% CI\")\n",
    "\n",
    "plt.xlabel(\"Duration (minutes)\", color=\"black\")\n",
    "plt.ylabel(\"Probability of Reprogramming\", color=\"black\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "for s in [\"bottom\", \"left\"]:\n",
    "    plt.gca().spines[s].set_color(\"black\")\n",
    "    plt.gca().spines[s].set_linewidth(1.2)\n",
    "for s in [\"top\", \"right\"]:\n",
    "    plt.gca().spines[s].set_visible(False)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.title(\"Effect of Duration on Probability of Reprogramming\", color=\"black\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af3e80-572d-4083-b529-ee370157a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Filter and quantile-bin the data ───────────────────────────────\n",
    "df_q = df[(df[\"duration_minutes\"] >= 30) & (df[\"duration_minutes\"] <= 105)].copy()\n",
    "df_q[\"duration_bin\"] = pd.qcut(df_q[\"duration_minutes\"], q=6, duplicates='drop')\n",
    "\n",
    "# Bin representative values (medians)\n",
    "bin_medians = df_q.groupby(\"duration_bin\")[\"duration_minutes\"].median()\n",
    "df_q[\"duration_group_value\"] = df_q[\"duration_bin\"].map(bin_medians)\n",
    "\n",
    "# ── 2. Spline basis and model fit ─────────────────────────────────────\n",
    "spline_basis = dmatrix(\"bs(duration_group_value, df=4, degree=3, include_intercept=False)\",\n",
    "                       data=df_q, return_type=\"dataframe\")\n",
    "X = sm.add_constant(spline_basis)\n",
    "y = df_q[\"reprog\"]\n",
    "\n",
    "model = sm.Logit(y, X).fit(disp=0)\n",
    "\n",
    "# ── 3. Prediction curve and CI over a grid ────────────────────────────\n",
    "grid = np.linspace(df_q[\"duration_group_value\"].min(),\n",
    "                   df_q[\"duration_group_value\"].max(), 300)\n",
    "grid_basis = dmatrix(\"bs(duration_group_value, df=4, degree=3, include_intercept=False)\",\n",
    "                     {\"duration_group_value\": grid}, return_type=\"dataframe\")\n",
    "X_grid = sm.add_constant(grid_basis)\n",
    "y_pred = model.predict(X_grid)\n",
    "\n",
    "# Confidence intervals\n",
    "cov = model.cov_params()\n",
    "se = np.sqrt(np.sum((X_grid @ cov) * X_grid, axis=1))\n",
    "logit_ci_low = model.predict(X_grid, which=\"linear\") - 1.96 * se\n",
    "logit_ci_high = model.predict(X_grid, which=\"linear\") + 1.96 * se\n",
    "ci_low = 1 / (1 + np.exp(-logit_ci_low))\n",
    "ci_high = 1 / (1 + np.exp(-logit_ci_high))\n",
    "\n",
    "# ── 4. Histogram data (bar plot for each bin) ─────────────────────────\n",
    "bin_counts = df_q[\"duration_bin\"].value_counts().sort_index()\n",
    "bar_x = bin_medians.values\n",
    "bar_heights = bin_counts.values\n",
    "\n",
    "# ── 5. Plot spline + observed means + histogram bars ──────────────────\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Main curve and confidence band\n",
    "ax1.plot(grid, y_pred, label=\"Predicted probability\", color=\"C0\")\n",
    "ax1.fill_between(grid, ci_low, ci_high, alpha=0.25, color=\"C0\", label=\"95% CI\")\n",
    "\n",
    "# Observed means per bin\n",
    "bin_means = df_q.groupby(\"duration_bin\")[\"reprog\"].mean()\n",
    "ax1.scatter(bar_x, bin_means.values, color=\"black\", label=\"Observed bin means\")\n",
    "\n",
    "ax1.set_xlabel(\"Duration (binned, median of each quantile)\")\n",
    "ax1.set_ylabel(\"P(reprog = 1)\", color=\"C0\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"C0\")\n",
    "\n",
    "# Secondary axis: histogram bars\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(bar_x, bar_heights, width=3, color=\"gray\", alpha=0.3, label=\"Bin counts\")\n",
    "ax2.set_ylabel(\"Show count (per bin)\", color=\"gray\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"gray\")\n",
    "ax2.set_ylim(0, max(bar_heights) * 1.2)\n",
    "\n",
    "# Final touches\n",
    "plt.title(\"Spline Model of Reprogramming by Duration (with Bin Histogram)\")\n",
    "fig.tight_layout()\n",
    "ax1.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339dbb9-0feb-46ec-a664-c25b832bc761",
   "metadata": {},
   "source": [
    "##### Group duration by bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19ba20-59a3-45f4-a41a-ff3061b9af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. Define duration bins and labels ─────────────────────────────\n",
    "bins = [29, 45, 60, 70, 85, 105]\n",
    "labels = [\"30–44\", \"45–59\", \"60–69\", \"70–84\", \"85–105\"]\n",
    "\n",
    "duration_clean[\"duration_group\"] = pd.cut(duration_clean[\"duration_minutes\"], bins=bins, labels=labels)\n",
    "\n",
    "# ── 3. Create dummy variables (reference = \"30–44\") ───────────────\n",
    "X = pd.get_dummies(duration_clean[\"duration_group\"], drop_first=True).astype(float)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = duration_clean[\"reprog\"]\n",
    "\n",
    "# ── 4. Fit logistic regression ─────────────────────────────────────\n",
    "model_duration_bins = sm.Logit(y, X).fit()\n",
    "print(model_duration_bins.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e7514-bf2d-4cc1-a04f-a215f146436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Odds ratios:\")\n",
    "print(np.exp(model_duration_bins.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01e57f-f9a2-41c2-aa76-4f29ac5df6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_by_group = duration_clean.groupby(\"duration_group\")[\"reprog\"].mean()\n",
    "print(probs_by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f40b1f-fffa-434f-98d4-637101188782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients and standard errors from the model\n",
    "coefs_duration_bins = model_duration_bins.params[1:]         # exclude intercept\n",
    "errors = model_duration_bins.bse[1:]\n",
    "labels = coefs_duration_bins.index\n",
    "\n",
    "# Compute odds ratios and CIs\n",
    "odds_ratios = np.exp(coefs_duration_bins)\n",
    "ci_low = np.exp(coefs_duration_bins - 1.96 * errors)\n",
    "ci_high = np.exp(coefs_duration_bins + 1.96 * errors)\n",
    "\n",
    "# Add the reference group (OR = 1)\n",
    "labels = [\"30–44\"] + list(labels)\n",
    "odds_ratios = [1.0] + list(odds_ratios)\n",
    "ci_low = [1.0] + list(ci_low)\n",
    "ci_high = [1.0] + list(ci_high)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.errorbar(labels[1:], odds_ratios[1:], \n",
    "             yerr=[np.array(odds_ratios[1:]) - np.array(ci_low[1:]), \n",
    "                   np.array(ci_high[1:]) - np.array(odds_ratios[1:])], \n",
    "             fmt='o', capsize=5, lw=2, label=\"Other durations\")\n",
    "\n",
    "# Add reference point manually\n",
    "plt.plot(labels[0], odds_ratios[0], 'o', color='gray', label=\"Reference (30–44 min)\")\n",
    "\n",
    "plt.axhline(1, color='gray', linestyle='--')\n",
    "plt.title(\"Odds Ratios of Reprogramming by Duration Group\")\n",
    "plt.ylabel(\"Odds Ratio (compared to 30–44 min)\")\n",
    "plt.xlabel(\"Duration Group (minutes)\")\n",
    "plt.ylim(bottom=0)\n",
    "plt.grid(True)\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa8bf2-3b40-42be-92c0-109a7d114ecd",
   "metadata": {},
   "source": [
    "#### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a2adc-4ea4-44b9-9ba3-df2fa115d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_map = {\n",
    "    # Théâtre / Café-théâtre\n",
    "    \"théâtre\": \"Théâtre / Café-théâtre\", \"café-théâtre\": \"Théâtre / Café-théâtre\",\n",
    "    \"comédie\": \"Théâtre / Café-théâtre\", \"drame\": \"Théâtre / Café-théâtre\",\n",
    "    \"boulevard\": \"Théâtre / Café-théâtre\", \"humour\": \"Théâtre / Café-théâtre\",\n",
    "    \"théâtre sonore\": \"Théâtre / Café-théâtre\",\n",
    "\n",
    "    # Spectacle musical / Concert\n",
    "    \"théâtre musical\": \"Spectacle musical / Concert\", \"spectacle musical\": \"Spectacle musical / Concert\",\n",
    "    \"concert\": \"Spectacle musical / Concert\", \"classique\": \"Spectacle musical / Concert\",\n",
    "    \"expo-concert\": \"Spectacle musical / Concert\", \"chanson\": \"Spectacle musical / Concert\",\n",
    "\n",
    "    # Danse / Danse-théâtre\n",
    "    \"danse\": \"Danse / Danse-théâtre\", \"danse-théâtre\": \"Danse / Danse-théâtre\",\n",
    "\n",
    "    # Cirque / Clown\n",
    "    \"cirque\": \"Cirque / Clown\", \"clown\": \"Cirque / Clown\",\n",
    "    \"magie\": \"Cirque / Clown\", \"théâtre musical/cirque\": \"Cirque / Clown\",\n",
    "\n",
    "    # Mime / Marionnettes-objets\n",
    "    \"mime\": \"Mime / Marionnettes-objets\",\n",
    "    \"marionnette-objet\": \"Mime / Marionnettes-objets\",\n",
    "    \"marionnette-objet de 7 mois à 4 ans\": \"Mime / Marionnettes-objets\",\n",
    "\n",
    "    # Conte / Poésie / Lecture\n",
    "    \"lecture\": \"Conte / Poésie / Lecture\", \"poésie\": \"Conte / Poésie / Lecture\",\n",
    "    \"conte\": \"Conte / Poésie / Lecture\", \"théâtre-poésie\": \"Conte / Poésie / Lecture\",\n",
    "}\n",
    "\n",
    "\n",
    "df[\"genre_group\"] = (\n",
    "    df[\"genre\"].str.lower().str.strip()\n",
    "      .str.replace(r\"\\s*/\\s*plein\\s+air\", \"\", regex=True)\n",
    "      .map(genre_map)\n",
    ")\n",
    "\n",
    "unmapped = df[df[\"genre_group\"].isna()]\n",
    "if not unmapped.empty:\n",
    "    print(\"⚠️ Warning: unmapped genre values detected:\\n\")\n",
    "    print(unmapped[\"genre\"].value_counts())\n",
    "\n",
    "print(\"✅ Mapped genre groups:\", df[\"genre_group\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01812cc5-b78c-4e7b-b60c-8945a71ab164",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_reprog_counts = (\n",
    "    df\n",
    "      .groupby([\"genre_group\", \"reprog\"])\n",
    "      .size()                         # count rows in each combination\n",
    "      .unstack(fill_value=0)          # turn reprog values into columns\n",
    "      .rename(columns={0: \"reprog_0\", 1: \"reprog_1\"})\n",
    "      .loc[[\n",
    "          \"Théâtre / Café-théâtre\",\n",
    "          \"Spectacle musical / Concert\",\n",
    "          \"Danse / Danse-théâtre\",\n",
    "          \"Cirque / Clown\",\n",
    "          \"Mime / Marionnettes-objets\",\n",
    "          \"Conte / Poésie / Lecture\"\n",
    "      ]]\n",
    ")\n",
    "\n",
    "print(genre_reprog_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe256f-3c39-456f-8039-a98c86d513ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_stats = (\n",
    "    df.groupby(\"genre_group\")[\"reprog\"]\n",
    "      .agg(total=\"count\", reprog_1=\"sum\")\n",
    "      .assign(percent=lambda d: 100 * d[\"reprog_1\"] / d[\"total\"])\n",
    "      .sort_values(\"percent\", ascending=False)\n",
    ")\n",
    "\n",
    "genre_stats[\"percent\"] = genre_stats[\"percent\"].round(1)\n",
    "\n",
    "print(\"🎭 Percentage of shows programmed (reprog = 1) by genre group:\\n\")\n",
    "display(genre_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eace78-03a9-45b8-9951-7b7d45cc3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Genre Grouped\n",
    "contingency = pd.crosstab(df[\"genre_group\"], df[\"reprog\"]).values\n",
    "n = contingency.sum()\n",
    "r, k = contingency.shape\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_val, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "# Uncorrected Cramér’s V\n",
    "phi2 = chi2 / n\n",
    "v_uncorrected = np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "# Bergsma-corrected Cramér’s V\n",
    "phi2_corr = max(0, phi2 - ((k - 1)*(r - 1))/(n - 1))\n",
    "r_corr = r - ((r - 1)**2)/(n - 1)\n",
    "k_corr = k - ((k - 1)**2)/(n - 1)\n",
    "v_corrected = np.sqrt(phi2_corr / min(k_corr - 1, r_corr - 1))\n",
    "\n",
    "# Bootstrap CI and permutation p-value\n",
    "n_iter = 5000\n",
    "boot_v_uncorr = []\n",
    "boot_v_corr = []\n",
    "perm_v = []\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    # Bootstrap\n",
    "    sample_idx = np.random.choice(range(contingency.shape[0]), size=contingency.shape[0], replace=True)\n",
    "    sample = contingency[sample_idx]\n",
    "    chi2_b, _, _, _ = chi2_contingency(sample)\n",
    "    n_b = sample.sum()\n",
    "    phi2_b = chi2_b / n_b\n",
    "    v_b_uncorr = np.sqrt(phi2_b / min(k - 1, r - 1))\n",
    "    phi2_b_corr = max(0, phi2_b - ((k - 1)*(r - 1))/(n_b - 1))\n",
    "    r_b_corr = r - ((r - 1)**2)/(n_b - 1)\n",
    "    k_b_corr = k - ((k - 1)**2)/(n_b - 1)\n",
    "    v_b_corr = np.sqrt(phi2_b_corr / min(k_b_corr - 1, r_b_corr - 1))\n",
    "\n",
    "    boot_v_uncorr.append(v_b_uncorr)\n",
    "    boot_v_corr.append(v_b_corr)\n",
    "\n",
    "    # Permutation\n",
    "    shuffled = df[\"reprog\"].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    perm_table = pd.crosstab(df[\"genre_group\"], shuffled).values\n",
    "    chi2_p, _, _, _ = chi2_contingency(perm_table)\n",
    "    phi2_p = chi2_p / perm_table.sum()\n",
    "    v_perm = np.sqrt(phi2_p / min(k - 1, r - 1))\n",
    "    perm_v.append(v_perm)\n",
    "\n",
    "# Confidence intervals\n",
    "ci_uncorr = np.percentile(boot_v_uncorr, [2.5, 97.5])\n",
    "ci_corr = np.percentile(boot_v_corr, [2.5, 97.5])\n",
    "\n",
    "# Permutation p-value\n",
    "perm_p_value = (np.sum(np.array(perm_v) >= v_uncorrected) + 1) / (len(perm_v) + 1)\n",
    "\n",
    "# Print results\n",
    "print(f\"Chi-square statistic      : {chi2:.3f}  (dof={dof}, p={p_val:.3f})\")\n",
    "print(f\"Cramér’s V (uncorrected)  : {v_uncorrected:.3f}  [95 % CI {ci_uncorr[0]:.3f} – {ci_uncorr[1]:.3f}]  Permutation p = {perm_p_value:.2e}\")\n",
    "print(f\"Cramér’s V (Bergsma corr.): {v_corrected:.3f}  [95 % CI {ci_corr[0]:.3f} – {ci_corr[1]:.3f}]  Permutation p = {perm_p_value:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11eb240-94f5-4d1a-bc76-900e4630b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"genre_group\"].astype(\"category\").cat.codes\n",
    "Y = df[\"reprog\"].astype(\"int\")\n",
    "\n",
    "# Mutual information I(X; Y)\n",
    "mi = mutual_info_score(X, Y)\n",
    "\n",
    "# Entropy H(Y)\n",
    "p_y = np.bincount(Y) / len(Y)\n",
    "h_y = entropy(p_y, base=2)\n",
    "\n",
    "# Compute Theil's U (Y | X)\n",
    "theils_u = mi / h_y if h_y != 0 else 0\n",
    "\n",
    "print(f\"Theil’s U (reprog | genre_group): {theils_u:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a08a8-f798-4f69-8c21-57a2fff8816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop missing data\n",
    "df_auc = df[[\"genre_group\", \"reprog\"]].dropna()\n",
    "\n",
    "# 2. Calculate mean reprog rate per genre\n",
    "probs = df_auc.groupby(\"genre_group\")[\"reprog\"].mean()\n",
    "\n",
    "# 3. Map the genre-based score back to each row\n",
    "df_auc[\"genre_score\"] = df_auc[\"genre_group\"].map(probs)\n",
    "\n",
    "# 4. Compute AUC using genre_score\n",
    "auc_value = roc_auc_score(df_auc[\"reprog\"], df_auc[\"genre_score\"])\n",
    "\n",
    "print(f\"AUC (genre_group): {auc_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f91095-a548-4b31-99a5-bc1bd895154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "genre_clean = df.dropna(subset=[\"genre_group\", \"reprog\"]).copy()\n",
    "genre_clean = genre_clean[genre_clean[\"genre_group\"] != \"Mime / Marionnettes-objets\"]\n",
    "\n",
    "# Create dummy variables from genre_group\n",
    "X = pd.get_dummies(genre_clean[\"genre_group\"], drop_first=True)\n",
    "\n",
    "# Add constant (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Target variable\n",
    "y = genre_clean[\"reprog\"].astype(int)\n",
    "X = X.astype(float)\n",
    "\n",
    "# Fit logistic regression model\n",
    "model_genre_group = sm.Logit(y, X).fit()\n",
    "print(model_genre_group .summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f31f52-c315-4439-ab92-e64f9d5ec7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_genre_group.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1a8a3-0b11-4f0b-8a4c-4c97862280e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Odds ratios:\")\n",
    "np.exp(model_genre_group.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28b537-0c18-46c2-87f1-7d850e1ce3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities per group\n",
    "genre_clean[\"predicted\"] = model_genre_group.predict(X)\n",
    "group_preds = genre_clean.groupby(\"genre_group\")[\"predicted\"].mean().reset_index()\n",
    "\n",
    "# Plot\n",
    "sns.barplot(x=\"genre_group\", y=\"predicted\", data=group_preds)\n",
    "plt.ylabel(\"Predicted probability of reprog\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Predicted reprog by genre_group\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4d393-3ff2-4f18-8566-df635906d087",
   "metadata": {},
   "source": [
    "##### Use raw genre categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4a037-e2f9-4783-a2dc-6dd0d6f5b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"genre_no_plein\"] = (\n",
    "    df[\"genre\"].str.lower().str.strip()\n",
    "      .str.replace(r\"\\s*/\\s*plein\\s+air\", \"\", regex=True)\n",
    ")\n",
    "df[\"genre_no_plein\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4b536-e708-41e9-a32d-75d19f7a8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total observations per genre\n",
    "merge_map = {\n",
    "    \"théâtre musical\": \"musical\",\n",
    "    \"spectacle musical\": \"musical\",\n",
    "    \"chanson\": \"concert\",\n",
    "    \"théâtre sonore\": \"musical\",\n",
    "    \"théâtre musical/cirque\": \"musical\",\n",
    "    \"marionnette-objet de 7 mois à 4 ans\":\"marionnette-objet\",\n",
    "    \"théâtre-poésie\":\"poésie\"\n",
    "}\n",
    "\n",
    "df[\"genre_merged\"] = df[\"genre_no_plein\"].replace(merge_map)\n",
    "\n",
    "# Count merged genres\n",
    "genre_counts = df[\"genre_merged\"].value_counts()\n",
    "\n",
    "# Filter to valid genres (at least 5 observations)\n",
    "valid_genres = genre_counts[genre_counts >= 10].index\n",
    "df_genre_filtered = df[df[\"genre_merged\"].isin(valid_genres)].copy()\n",
    "\n",
    "print(df_genre_filtered[\"genre_merged\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5174127-ec8b-4ec3-974b-423e7032ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (\n",
    "    df_genre_filtered.groupby(\"genre_merged\")[\"reprog\"]\n",
    "      .agg(total=\"count\", reprog_1=\"sum\")\n",
    "      .assign(percent=lambda d: 100 * d[\"reprog_1\"] / d[\"total\"])\n",
    "      .sort_values(\"total\", ascending=False)\n",
    ")\n",
    "\n",
    "tmp[\"percent\"] = tmp[\"percent\"].round(1)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  # see all rows\n",
    "\n",
    "print(\"🏛️ Percentage of shows programmed (reprog = 1) by genre :\\n\")\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7bb4d-0671-4236-8f91-96613f493c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build contingency table\n",
    "contingency = pd.crosstab(df_genre_filtered[\"genre_merged\"], df_genre_filtered[\"reprog\"]).values\n",
    "n = contingency.sum()\n",
    "r, k = contingency.shape\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_val, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "# Uncorrected Cramér’s V\n",
    "phi2 = chi2 / n\n",
    "v_uncorrected = np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "# Bergsma-corrected Cramér’s V\n",
    "phi2_corr = max(0, phi2 - ((k - 1)*(r - 1))/(n - 1))\n",
    "r_corr = r - ((r - 1)**2)/(n - 1)\n",
    "k_corr = k - ((k - 1)**2)/(n - 1)\n",
    "v_corrected = np.sqrt(phi2_corr / min(k_corr - 1, r_corr - 1))\n",
    "\n",
    "# Bootstrap CI and permutation p-value\n",
    "n_iter = 5000\n",
    "boot_v_uncorr = []\n",
    "boot_v_corr = []\n",
    "perm_v = []\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    # Bootstrap\n",
    "    sample_idx = np.random.choice(range(contingency.shape[0]), size=contingency.shape[0], replace=True)\n",
    "    sample = contingency[sample_idx]\n",
    "    chi2_b, _, _, _ = chi2_contingency(sample)\n",
    "    n_b = sample.sum()\n",
    "    phi2_b = chi2_b / n_b\n",
    "    v_b_uncorr = np.sqrt(phi2_b / min(k - 1, r - 1))\n",
    "    phi2_b_corr = max(0, phi2_b - ((k - 1)*(r - 1))/(n_b - 1))\n",
    "    r_b_corr = r - ((r - 1)**2)/(n_b - 1)\n",
    "    k_b_corr = k - ((k - 1)**2)/(n_b - 1)\n",
    "    v_b_corr = np.sqrt(phi2_b_corr / min(k_b_corr - 1, r_b_corr - 1))\n",
    "\n",
    "    boot_v_uncorr.append(v_b_uncorr)\n",
    "    boot_v_corr.append(v_b_corr)\n",
    "\n",
    "    # Permutation\n",
    "    shuffled = df_genre_filtered[\"reprog\"].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    perm_table = pd.crosstab(df_genre_filtered[\"genre_merged\"], shuffled).values\n",
    "    chi2_p, _, _, _ = chi2_contingency(perm_table)\n",
    "    phi2_p = chi2_p / perm_table.sum()\n",
    "    v_perm = np.sqrt(phi2_p / min(k - 1, r - 1))\n",
    "    perm_v.append(v_perm)\n",
    "\n",
    "# Confidence intervals\n",
    "ci_uncorr = np.percentile(boot_v_uncorr, [2.5, 97.5])\n",
    "ci_corr = np.percentile(boot_v_corr, [2.5, 97.5])\n",
    "\n",
    "# Permutation p-value\n",
    "perm_p_value = (np.sum(perm_v >= v_uncorrected) + 1) / (len(perm_v) + 1)\n",
    "\n",
    "# Print results\n",
    "print(f\"Chi-square statistic      : {chi2:.3f}  (dof={dof}, p={p_val:.3f})\")\n",
    "print(f\"Cramér’s V (uncorrected)  : {v_uncorrected:.3f}  [95 % CI {ci_uncorr[0]:.3f} – {ci_uncorr[1]:.3f}]  Permutation p = {perm_p_value:.5f}\")\n",
    "print(f\"Cramér’s V (Bergsma corr.): {v_corrected:.3f}  [95 % CI {ci_corr[0]:.3f} – {ci_corr[1]:.3f}]  Permutation p = {perm_p_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407ccff-8888-47fa-b5c6-ae18eb724ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_genre_filtered[\"genre_merged\"].astype(\"category\").cat.codes\n",
    "Y = df_genre_filtered[\"reprog\"].astype(\"int\")\n",
    "\n",
    "mi = mutual_info_score(X, Y)\n",
    "\n",
    "# Entropy H(Y)\n",
    "p_y = np.bincount(Y) / len(Y)\n",
    "h_y = entropy(p_y, base=2)\n",
    "\n",
    "# Compute Theil's U (Y | X)\n",
    "theils_u = mi / h_y if h_y != 0 else 0\n",
    "\n",
    "print(f\"Theil’s U (reprog | genre_group): {theils_u:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294fa92-070b-4645-a124-ddf79ecbb0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auc = df_genre_filtered[[\"genre_merged\", \"reprog\"]].dropna()\n",
    "\n",
    "# 2. Compute mean reprog rate per genre\n",
    "genre_probs = df_auc.groupby(\"genre_merged\")[\"reprog\"].mean()\n",
    "\n",
    "# 3. Map back to each row\n",
    "df_auc[\"genre_score\"] = df_auc[\"genre_merged\"].map(genre_probs)\n",
    "\n",
    "# 4. Compute AUC\n",
    "auc_value = roc_auc_score(df_auc[\"reprog\"], df_auc[\"genre_score\"])\n",
    "\n",
    "print(f\"AUC (genre_merged): {auc_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a41bb2-c2a3-44e2-93e7-0b97ba42fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_genre = pd.crosstab(df_genre_filtered[\"genre_merged\"], df_genre_filtered[\"reprog\"])\n",
    "tbl_genre.columns = [\"reprog_0\", \"reprog_1\"]\n",
    "\n",
    "# ── 2. Filter genres with enough examples ──────────────────────────\n",
    "mask = (tbl_genre[\"reprog_0\"] >= 5) & (tbl_genre[\"reprog_1\"] >= 2)\n",
    "valid_genres = tbl_genre[mask].index.tolist()\n",
    "\n",
    "dropped_genres = tbl_genre[~mask]\n",
    "print(\"Dropped genres:\\n\", dropped_genres)\n",
    "\n",
    "df_genre_clean = df_genre_filtered[df_genre_filtered[\"genre_merged\"].isin(valid_genres)].copy()\n",
    "\n",
    "# ── 3. Fit logistic regression ─────────────────────────────────────\n",
    "X = pd.get_dummies(df_genre_clean[\"genre_merged\"], drop_first=True).astype(float)\n",
    "X = sm.add_constant(X)\n",
    "y = df_genre_clean[\"reprog\"].astype(int)\n",
    "\n",
    "model_genre = sm.Logit(y, X).fit()\n",
    "print(model_genre.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90641ab3-097e-4ff3-9209-4f9b4ff91968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_genre.aic)\n",
    "np.exp(model_genre.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597625e1-5fa7-4c13-a642-a1fc78cf57da",
   "metadata": {},
   "source": [
    "##### vizualize pred / genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2720d20-4c92-4f6e-9c92-d6f9a4feafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = model_genre.predict(X)\n",
    "pred_logit = model_genre.predict(X, which=\"linear\")\n",
    "\n",
    "# Compute standard errors for confidence intervals\n",
    "cov = model_genre.cov_params()\n",
    "se = np.sqrt(np.sum((X @ cov) * X, axis=1))\n",
    "\n",
    "logit_ci_low = pred_logit - 1.96 * se\n",
    "logit_ci_high = pred_logit + 1.96 * se\n",
    "prob_ci_low = 1 / (1 + np.exp(-logit_ci_low))\n",
    "prob_ci_high = 1 / (1 + np.exp(-logit_ci_high))\n",
    "\n",
    "# Combine predictions with genre labels\n",
    "df_preds = df_genre_clean[[\"genre_merged\"]].copy()\n",
    "df_preds[\"pred_prob\"] = pred_prob\n",
    "df_preds[\"ci_low\"] = prob_ci_low\n",
    "df_preds[\"ci_high\"] = prob_ci_high\n",
    "\n",
    "# Aggregate to one row per genre\n",
    "grouped = df_preds.groupby(\"genre_merged\", as_index=False).agg({\n",
    "    \"pred_prob\": \"mean\",\n",
    "    \"ci_low\": \"mean\",\n",
    "    \"ci_high\": \"mean\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe489d-2d21-4990-9a9d-e2a94da2f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort genres by predicted probability\n",
    "grouped = grouped.sort_values(\"pred_prob\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot points with error bars\n",
    "plt.errorbar(\n",
    "    grouped[\"pred_prob\"],\n",
    "    grouped[\"genre_merged\"],\n",
    "    xerr=[\n",
    "        grouped[\"pred_prob\"] - grouped[\"ci_low\"],\n",
    "        grouped[\"ci_high\"] - grouped[\"pred_prob\"]\n",
    "    ],\n",
    "    fmt='o',\n",
    "    color=\"darkred\",\n",
    "    ecolor=\"darkorange\",\n",
    "    elinewidth=1.5,\n",
    "    capsize=4,\n",
    "    markersize=6\n",
    ")\n",
    "\n",
    "# Axes and labels\n",
    "plt.xlabel(\"Pred. of Reprogr.\", color=\"black\")\n",
    "plt.ylabel(\"Genre\", color=\"black\")\n",
    "plt.title(\"Estimated Probability of Reprogramming by Genre\", color=\"black\")\n",
    "\n",
    "# Styling\n",
    "ax = plt.gca()\n",
    "ax.spines[\"left\"].set_color(\"black\")\n",
    "ax.spines[\"bottom\"].set_color(\"black\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.tick_params(axis='x', colors='black')\n",
    "ax.tick_params(axis='y', colors='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "\n",
    "# Save as PNG and PDF\n",
    "png_path = os.path.join(FIG_PATH, \"genre_pred_new2.png\")\n",
    "pdf_path = os.path.join(FIG_PATH, \"genre_pred_new2.pdf\")\n",
    "plt.savefig(png_path, dpi=600, bbox_inches=\"tight\", transparent=False)\n",
    "plt.savefig(pdf_path, dpi=600, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87accd9-ae90-49a5-b179-e983dca48e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for valid genre_group entries\n",
    "tbl_grouped = pd.crosstab(df[\"genre_group\"], df[\"reprog\"])\n",
    "keep_mask = (tbl_grouped.sum(axis=1) >= 5) & (tbl_grouped[1] > 2)\n",
    "valid_genres_grouped = tbl_grouped[keep_mask].index\n",
    "df_grouped = df[df[\"genre_group\"].isin(valid_genres_grouped)].copy()\n",
    "\n",
    "# Heatmap (% reprog per genre_group)\n",
    "heatmap_data = pd.crosstab(df_grouped[\"genre_group\"], df_grouped[\"reprog\"], normalize='index') * 100\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"YlOrBr\", cbar_kws={'label': '% Reprog'})\n",
    "plt.title(\"Reprogramming Rate by Grouped Genre (Heatmap %)\")\n",
    "plt.xlabel(\"Reprogrammed (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Genre Group\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff3bc9-3b05-4d65-940f-753ef764dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Heatmap of % reprog per genre_merged\n",
    "heatmap_data = pd.crosstab(df_genre_clean[\"genre_merged\"], df_genre_clean[\"reprog\"], normalize='index') * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"YlOrBr\",  # You can replace with \"YlGnBu\" or \"PuRd\" for different tones\n",
    "    cbar_kws={'label': '% Rate'}\n",
    ")\n",
    "plt.title(\"Rate by Genre (%)\")\n",
    "plt.xlabel(\"Reprogrammed in Paris\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.tight_layout()\n",
    "png_path = os.path.join(FIG_PATH, \"heatmap_genre_reprog.png\")\n",
    "pdf_path = os.path.join(FIG_PATH, \"heatmap_genre_reprog.pdf\")\n",
    "\n",
    "# Save in high resolution for print (PNG) and as vector (PDF)\n",
    "plt.savefig(png_path, dpi=600, bbox_inches=\"tight\", transparent=False)\n",
    "plt.savefig(pdf_path, dpi=600, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16fb5a-1c81-4767-9266-f3384bf13331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels (categorical genre), predicted binary outcome\n",
    "labels_true = df[\"genre_group\"]\n",
    "labels_pred = df[\"reprog\"]\n",
    "\n",
    "# Compute MI metrics\n",
    "raw_mi = mutual_info_score(labels_true, labels_pred)\n",
    "norm_mi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "adj_mi = adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "print(f\"Mutual Information       : {raw_mi:.3f}\")\n",
    "print(f\"Normalized MI            : {norm_mi:.3f}\")\n",
    "print(f\"Adjusted Mutual Info     : {adj_mi:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e960b7-6b5b-4287-b13a-f90565d9fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels (categorical genre), predicted binary outcome\n",
    "labels_true = df[\"genre_merged\"]\n",
    "labels_pred = df[\"reprog\"]\n",
    "\n",
    "# Compute MI metrics\n",
    "raw_mi = mutual_info_score(labels_true, labels_pred)\n",
    "norm_mi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "adj_mi = adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "print(f\"Mutual Information       : {raw_mi:.3f}\")\n",
    "print(f\"Normalized MI            : {norm_mi:.3f}\")\n",
    "print(f\"Adjusted Mutual Info     : {adj_mi:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36400b30-6233-4be4-91af-ec68ed6e3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels (categorical genre), predicted binary outcome\n",
    "labels_true = df_genre_clean[\"genre_merged\"]\n",
    "labels_pred = df_genre_clean[\"reprog\"]\n",
    "\n",
    "# Compute MI metrics\n",
    "raw_mi = mutual_info_score(labels_true, labels_pred)\n",
    "norm_mi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "adj_mi = adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "print(f\"Mutual Information       : {raw_mi:.3f}\")\n",
    "print(f\"Normalized MI            : {norm_mi:.3f}\")\n",
    "print(f\"Adjusted Mutual Info     : {adj_mi:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09741a-b2d1-4151-b7e6-2084e0f315fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode genre and reprog as integers\n",
    "X = df_genre_clean[\"genre_merged\"].astype(\"category\").cat.codes\n",
    "Y = df_genre_clean[\"reprog\"].astype(\"int\")\n",
    "\n",
    "mi = mutual_info_score(X, Y)\n",
    "\n",
    "# Entropy H(Y)\n",
    "p_y = np.bincount(Y) / len(Y)\n",
    "h_y = entropy(p_y, base=2)\n",
    "\n",
    "# Compute Theil's U (Y | X)\n",
    "theils_u = mi / h_y if h_y != 0 else 0\n",
    "\n",
    "print(f\"Theil’s U (reprog | genre): {theils_u:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d52260-a2ab-4c43-a1ba-2212090629f0",
   "metadata": {},
   "source": [
    "#### Theater Venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b3df4-949e-479e-bc48-ca6b82ff4d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "venues = (\n",
    "    df.groupby(\"festival_theater\")[\"reprog\"]\n",
    "      .agg(total=\"count\", reprog_1=\"sum\")\n",
    "      .assign(percent=lambda d: 100 * d[\"reprog_1\"] / d[\"total\"])\n",
    "      .sort_values(\"percent\", ascending=False)\n",
    ")\n",
    "\n",
    "# 2. Round for cleaner output\n",
    "venues[\"percent\"] = venues[\"percent\"].round(1)\n",
    "\n",
    "# 3. Display full table\n",
    "pd.set_option(\"display.max_rows\", None)  # optional: see all rows\n",
    "print(\"🏛️ Percentage of shows programmed (reprog = 1) by venue:\\n\")\n",
    "display(venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1521c04-039e-433e-b63b-3d971e4f96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265d600-4aa6-4d91-8032-e604161be4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4aa52-6c81-494a-8715-0a7e1afd609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_venues_count = df[\"festival_theater\"].nunique()\n",
    "print(f\"🎭 Number of unique venues before grouping: {unique_venues_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f87d2-afd1-4b64-8355-370cefb6e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep venues with ≥6 total shows (regardless of reprog signal)\n",
    "\n",
    "\n",
    "tbl_venue = pd.crosstab(df[\"festival_theater\"], df[\"reprog\"])\n",
    "tbl_venue.columns = [\"reprog_0\", \"reprog_1\"]\n",
    "tbl_venue[\"total\"] = tbl_venue.sum(axis=1)\n",
    "\n",
    "# Keep venues with ≥6 shows and ≥1 reprog\n",
    "valid_venues = tbl_venue[tbl_venue[\"total\"] >= 6].index\n",
    "\n",
    "# Still keep rare but nonzero-signal venues\n",
    "small_signal_venues = tbl_venue[\n",
    "    (tbl_venue[\"total\"] < 6)\n",
    "].index\n",
    "\n",
    "def group_venue(v):\n",
    "    if v in valid_venues:\n",
    "        return v\n",
    "    else :\n",
    "        return \"Small Venues\"\n",
    "\n",
    "df_venues=df.copy()\n",
    "df_venues[\"venue_grouped\"] = df_venues[\"festival_theater\"].apply(group_venue)\n",
    "df_venues = df_venues[df_venues[\"venue_grouped\"].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2503ae6-aae6-4329-b1fe-006cb000639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues[\"venue_grouped\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47f316-9230-494e-8478-fc0a094007bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bd84e-14d1-4fc6-a425-19407c3883e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Venues grouped under 'Small Venues':\")\n",
    "len(small_signal_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d996a-4c01-440b-b1d6-97ab102f888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871c939-ebcc-4eba-b6f5-5a36e7325751",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_venues_count = df_venues[\"venue_grouped\"].nunique()\n",
    "print(f\"🎭 Number of unique venues after grouping: {unique_venues_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93293a54-6dfc-4eb4-88d1-710b26fb1576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Aggregate reprogramming stats by grouped venue\n",
    "venues = (\n",
    "    df_venues.groupby(\"venue_grouped\")[\"reprog\"]\n",
    "      .agg(total=\"count\", reprog_1=\"sum\")\n",
    "      .assign(percent=lambda d: 100 * d[\"reprog_1\"] / d[\"total\"])\n",
    "      .sort_values(\"percent\", ascending=False)\n",
    ")\n",
    "\n",
    "# 3. Round for cleaner display\n",
    "venues[\"percent\"] = venues[\"percent\"].round(1)\n",
    "\n",
    "# 4. Display full table\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(\"🏛️ Percentage of shows programmed (reprog = 1) by grouped venue:\\n\")\n",
    "display(venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fca810-40f8-4dc2-93cb-a9b79c64f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1.  Contingency table (venue vs reprog) ────────────────────────\n",
    "ct = pd.crosstab(df_venues[\"venue_grouped\"], df_venues[\"reprog\"]).values\n",
    "n  = ct.sum()\n",
    "r, k = ct.shape\n",
    "\n",
    "# ── 2.  Chi-square and Cramér’s V (raw + corrected) ────────────────\n",
    "chi2, p_chi2, dof, _ = chi2_contingency(ct)\n",
    "\n",
    "phi2   = chi2 / n\n",
    "v_raw  = np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "phi2_c = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))      # Bergsma correction\n",
    "r_corr = r - ((r - 1) ** 2) / (n - 1)\n",
    "k_corr = k - ((k - 1) ** 2) / (n - 1)\n",
    "v_corr = np.sqrt(phi2_c / min(k_corr - 1, r_corr - 1))\n",
    "\n",
    "# ── 3.  Bootstrap CIs  & permutation p-value ───────────────────────\n",
    "B = 5000\n",
    "boot_raw, boot_corr, perm_raw = [], [], []\n",
    "\n",
    "for _ in range(B):\n",
    "    # bootstrap rows of df\n",
    "    samp_df = df_venues.sample(len(df_venues), replace=True)\n",
    "    samp_ct = pd.crosstab(samp_df[\"venue_grouped\"], samp_df[\"reprog\"]).values\n",
    "    chi2_b, _, _, _ = chi2_contingency(samp_ct)\n",
    "    phi2_b   = chi2_b / samp_ct.sum()\n",
    "    boot_raw.append(np.sqrt(phi2_b / min(k - 1, r - 1)))\n",
    "\n",
    "    phi2_b_c = max(0, phi2_b - ((k - 1)*(r - 1)) / (samp_ct.sum() - 1))\n",
    "    boot_corr.append(\n",
    "        np.sqrt(phi2_b_c / min(k_corr - 1, r_corr - 1))\n",
    "    )\n",
    "\n",
    "    # permutation\n",
    "    shuffled = df_venues[\"reprog\"].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    perm_ct  = pd.crosstab(df_venues[\"venue_grouped\"], shuffled).values\n",
    "    chi2_p, _, _, _ = chi2_contingency(perm_ct)\n",
    "    phi2_p = chi2_p / perm_ct.sum()\n",
    "    perm_raw.append(np.sqrt(phi2_p / min(k - 1, r - 1)))\n",
    "\n",
    "# CIs (2.5 % – 97.5 %)\n",
    "ci_raw  = np.percentile(boot_raw,  [2.5, 97.5])\n",
    "ci_corr = np.percentile(boot_corr, [2.5, 97.5])\n",
    "\n",
    "# permutation p-value (+1 correction)\n",
    "perm_p_raw = (np.sum(np.array(perm_raw) >= v_raw) + 1) / (B + 1)\n",
    "\n",
    "# ── 4.  Print results ──────────────────────────────────────────────\n",
    "print(f\"χ² = {chi2:.3f}  (dof={dof}, p = {p_chi2:.2e})\")\n",
    "print(f\"Cramér’s V (raw)  : {v_raw:.3f}  [95 % CI {ci_raw[0]:.3f} – {ci_raw[1]:.3f}]  \"\n",
    "      f\"perm-p = {perm_p_raw:.4f}\")\n",
    "print(f\"Cramér’s V (corr) : {v_corr:.3f}  [95 % CI {ci_corr[0]:.3f} – {ci_corr[1]:.3f}]  \"\n",
    "      f\"perm-p = {perm_p_raw:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be71540-3167-41e0-8ce8-52122ce46faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_venues[\"venue_grouped\"].astype(\"category\").cat.codes\n",
    "Y = df_venues[\"reprog\"].astype(\"int\")\n",
    "\n",
    "mi = mutual_info_score(X, Y)\n",
    "\n",
    "# Entropy H(Y)\n",
    "p_y = np.bincount(Y) / len(Y)\n",
    "h_y = entropy(p_y, base=2)\n",
    "\n",
    "# Compute Theil's U (Y | X)\n",
    "theils_u = mi / h_y if h_y != 0 else 0\n",
    "\n",
    "print(f\"Theil’s U (reprog | genre_group): {theils_u:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61485de-cf99-46ee-8d0e-0f46a9740563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 1. Drop missing data\n",
    "df_auc = df_venues[[\"venue_grouped\", \"reprog\"]].dropna()\n",
    "\n",
    "# 2. Calculate mean reprog rate per venue (acts as a predicted \"score\")\n",
    "venue_probs = df_auc.groupby(\"venue_grouped\")[\"reprog\"].mean()\n",
    "\n",
    "# 3. Map back to the full DataFrame\n",
    "df_auc[\"venue_score\"] = df_auc[\"venue_grouped\"].map(venue_probs)\n",
    "\n",
    "# 4. Compute AUC\n",
    "auc_value = roc_auc_score(df_auc[\"reprog\"], df_auc[\"venue_score\"])\n",
    "\n",
    "print(f\"AUC (venue_grouped): {auc_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c25f0-2854-4771-8d4b-d1da81405f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "tbl_venue = pd.crosstab(df_venues[\"venue_grouped\"], df_venues[\"reprog\"])\n",
    "tbl_venue.columns = [\"reprog_0\", \"reprog_1\"]\n",
    "\n",
    "# 2. Keep only venues where at least 1 show was reprogrammed\n",
    "venues_with_reprog = tbl_venue[tbl_venue[\"reprog_1\"] > 0].index\n",
    "\n",
    "# 3. Filter df_venues\n",
    "df_pred = df_venues[df_venues[\"venue_grouped\"].isin(venues_with_reprog)].copy()\n",
    "X = pd.get_dummies(df_pred[\"venue_grouped\"], drop_first=True).astype(float)\n",
    "X = sm.add_constant(X)\n",
    "y = df_pred[\"reprog\"].astype(int)\n",
    "\n",
    "# Fit logistic regression\n",
    "model_venues = sm.Logit(y, X).fit()\n",
    "print(model_venues.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8cc6f-2868-4a64-ab3a-1849a2b45535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print odds ratios\n",
    "print(\"AIC:\", model_venues.aic)\n",
    "print(\"\\nOdds ratios:\")\n",
    "print(np.exp(model_venues.params).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce7099-a780-4a38-8237-a99808f1b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model_venues.summary2().tables[1]\n",
    "summary_sorted = summary.sort_values(\"P>|z|\")\n",
    "summary_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d68f5-db0c-4a00-9183-92842e2de7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for statistically significant results\n",
    "significant = summary[summary[\"P>|z|\"] < 0.05]\n",
    "\n",
    "# Display\n",
    "print(\"Statistically significant theater venues (p < 0.05):\")\n",
    "print(significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7611aea-0644-47e9-80af-54ea6f9c891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_venues.get_prediction(X)\n",
    "summary_frame = predictions.summary_frame(alpha=0.05)\n",
    "grouped = df_pred[\"venue_grouped\"].to_frame().copy()\n",
    "grouped[\"pred_prob\"] = summary_frame[\"predicted\"]\n",
    "grouped[\"ci_lower\"] = summary_frame[\"ci_lower\"]\n",
    "grouped[\"ci_upper\"] = summary_frame[\"ci_upper\"]\n",
    "\n",
    "grouped = grouped.groupby(\"venue_grouped\").agg(\n",
    "    pred_prob=(\"pred_prob\", \"mean\"),\n",
    "    ci_low=(\"ci_lower\", \"mean\"),\n",
    "    ci_high=(\"ci_upper\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Sort by predicted probability (optional)\n",
    "grouped = grouped.sort_values(\"pred_prob\", ascending=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, len(grouped) * 0.3))\n",
    "plt.errorbar(\n",
    "    grouped[\"pred_prob\"],\n",
    "    grouped[\"venue_grouped\"],\n",
    "    xerr=[\n",
    "        grouped[\"pred_prob\"] - grouped[\"ci_low\"],\n",
    "        grouped[\"ci_high\"] - grouped[\"pred_prob\"]\n",
    "    ],\n",
    "    fmt='o',\n",
    "    color=\"darkred\",\n",
    "    ecolor=\"darkred\",\n",
    "    elinewidth=1.3,\n",
    "    capsize=3,\n",
    "    markersize=5\n",
    ")\n",
    "\n",
    "# Aesthetic tweaks\n",
    "plt.xlabel(\"Predicted Probability of Reprogramming\")\n",
    "plt.ylabel(\"Venue\")\n",
    "plt.title(\"Predicted Probability of Reprogramming by Venue (with 95% CI)\")\n",
    "plt.tick_params(axis='both', colors='black')\n",
    "plt.grid(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_color('black')\n",
    "plt.gca().spines['bottom'].set_color('black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8b657-926c-4219-baae-d98eede6ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to the first 8 venues after sorting\n",
    "top_venues = grouped.tail(12)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, len(top_venues) * 0.5))\n",
    "plt.errorbar(\n",
    "    top_venues[\"pred_prob\"],\n",
    "    top_venues[\"venue_grouped\"],\n",
    "    xerr=[\n",
    "        top_venues[\"pred_prob\"] - top_venues[\"ci_low\"],\n",
    "        top_venues[\"ci_high\"] - top_venues[\"pred_prob\"]\n",
    "    ],\n",
    "    fmt='o',\n",
    "    color=\"darkred\",        # dot color\n",
    "    ecolor=\"orange\",        # CI bar color\n",
    "    elinewidth=1.3,\n",
    "    capsize=3,\n",
    "    markersize=5\n",
    ")\n",
    "\n",
    "# Aesthetic tweaks\n",
    "plt.xlabel(\"Pred. Probability of Reprog.\")\n",
    "plt.ylabel(\"Venue\")\n",
    "plt.title(\"Top 8 Venues by Predicted Reprogramming Probability (with 95% CI)\")\n",
    "plt.tick_params(axis='both', colors='black')\n",
    "plt.grid(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_color('black')\n",
    "plt.gca().spines['bottom'].set_color('black')\n",
    "plt.tight_layout()\n",
    "\n",
    "png_path = os.path.join(FIG_PATH, \"reprog_venues.png\")\n",
    "pdf_path = os.path.join(FIG_PATH, \"reprog_venues.pdf\")\n",
    "\n",
    "plt.savefig(png_path, dpi=300, bbox_inches='tight')  # PNG with high resolution\n",
    "plt.savefig(pdf_path, bbox_inches='tight')           # PDF (vector quality)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0cef8-5df4-4307-a8db-5ebfa2de30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC and BIC comparison \n",
    "print(\"🟦 Duration-only model:\")\n",
    "print(f\"AIC: {model_duration.aic:.2f}\")\n",
    "print(f\"BIC: {model_duration.bic:.2f}\\n\")\n",
    "\n",
    "print(\"🟩 Duration-grouped model:\")\n",
    "print(f\"AIC: {model_duration_bins.aic:.2f}\")\n",
    "print(f\"BIC: {model_duration_bins.bic:.2f}\\n\")\n",
    "\n",
    "print(\"🟪 Actors-only model :\")\n",
    "print(f\"AIC: {model_act.aic:.2f}\")\n",
    "print(f\"BIC: {model_act.bic:.2f}\\n\")\n",
    "\n",
    "print(\"⬛⬜🟫   Genre-raw model:\")\n",
    "print(f\"AIC: {model_genre.aic:.2f}\")\n",
    "print(f\"BIC: {model_genre.bic:.2f}\\n\")\n",
    "\n",
    "print(\"🟨 Genre-grouped model:\")\n",
    "print(f\"AIC: {model_genre_group.aic:.2f}\")\n",
    "print(f\"BIC: {model_genre_group.bic:.2f}\\n\")\n",
    "\n",
    "print(\"🟥 Venues-only model:\")\n",
    "print(f\"AIC: {model_venues.aic:.2f}\")\n",
    "print(f\"BIC: {model_venues.bic:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de46d9-cb00-4649-8376-db2444410693",
   "metadata": {},
   "source": [
    "#### Combined Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15897022-a2f2-42fc-9a5e-789571007c81",
   "metadata": {},
   "source": [
    "##### Duration and venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ab78f-167c-4789-a0de-972af206e431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ── 2. Filter to valid duration range ─────────────────────────────────\n",
    "df_pred = df_pred[df_pred[\"venue_grouped\"].isin(venues_with_reprog)].copy()\n",
    "\n",
    "df_pred = df_pred[(df_pred[\"duration_minutes\"] >= 30) & (df_pred[\"duration_minutes\"] <= 105)].copy()\n",
    "\n",
    "# ── 3. Spline basis for duration ──────────────────────────────────────\n",
    "spline_basis = dmatrix(\"bs(duration_minutes, df=4, degree=3, include_intercept=False)\",\n",
    "                       data=df_pred, return_type='dataframe')\n",
    "\n",
    "venue_dummies = pd.get_dummies(df_pred[\"venue_grouped\"], drop_first=True).astype(float)\n",
    "\n",
    "# ── 5. Combine features into design matrix ────────────────────────────\n",
    "X = pd.concat([spline_basis, venue_dummies], axis=1)\n",
    "X = sm.add_constant(X)\n",
    "y = df_pred[\"reprog\"].astype(int)\n",
    "\n",
    "# ── 6. Fit logistic regression ────────────────────────────────────────\n",
    "model_venues_dur = sm.Logit(y, X).fit()\n",
    "print(model_venues_dur.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6675a9-8e12-4f39-971e-2aa22fb4be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_venues_dur.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffd715-32aa-4f59-8342-89bbd5ef690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients and p-values\n",
    "summary = model_venues_dur.summary2().tables[1]\n",
    "summary = summary.reset_index().rename(columns={\"index\": \"terms\", \"Coef.\": \"coef\", \"P>|z|\": \"pval\"})\n",
    "\n",
    "# Filter out intercept\n",
    "summary = summary[summary[\"terms\"] != \"const\"]\n",
    "\n",
    "# Top 10 venues by coefficient\n",
    "top10 = summary.sort_values(\"pval\", ascending=True).head(15)\n",
    "print(\"Top 15 terms by estimated effect:\")\n",
    "top10[[\"terms\", \"coef\", \"pval\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d22f1-4477-4b91-a731-a9cb58ee333b",
   "metadata": {},
   "source": [
    "##### Duration and raw genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572b68f-3330-40a1-b0fa-5dad774eb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Constant or near-constant columns:\")\n",
    "print((X.nunique() == 1).sum(), \"columns with constant values\")\n",
    "\n",
    "print(\"🔍 Columns with only one unique value:\")\n",
    "print(X.loc[:, X.nunique() == 1].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722995f-a577-4669-b255-64c139385059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pred = df[(df[\"duration_minutes\"] >= 30) & (df[\"duration_minutes\"] <= 105)].copy()\n",
    "\n",
    "# ── 4. Compute genre frequency table ───────────────────────────────────\n",
    "tbl_genre = pd.crosstab(df_pred[\"genre_merged\"], df_pred[\"reprog\"])\n",
    "tbl_genre.columns = [\"reprog_0\", \"reprog_1\"]\n",
    "\n",
    "mask = (tbl_genre[\"reprog_0\"] >= 5) & (tbl_genre[\"reprog_1\"] >= 1) \n",
    "valid_genres = tbl_genre[mask].index\n",
    "df_pred = df_pred[df_pred[\"genre_merged\"].isin(valid_genres)].copy()\n",
    "\n",
    "# ── 7. Create spline basis for duration ────────────────────────────────\n",
    "spline_duration = dmatrix(\n",
    "    \"bs(duration_minutes, df=4, degree=3, include_intercept=False)\",\n",
    "    data=df_pred,\n",
    "    return_type='dataframe'\n",
    ")\n",
    "\n",
    "# ── 8. Create dummy variables for genre ────────────────────────────────\n",
    "genre_dummies = pd.get_dummies(df_pred[\"genre_merged\"], drop_first=True).astype(float)\n",
    "\n",
    "# ── 9. Combine predictors ──────────────────────────────────────────────\n",
    "X = pd.concat([spline_duration.reset_index(drop=True), genre_dummies.reset_index(drop=True)], axis=1)\n",
    "X = sm.add_constant(X)\n",
    "y = df_pred[\"reprog\"].astype(int).reset_index(drop=True)\n",
    "\n",
    "# ── 10. Fit logistic regression ────────────────────────────────────────\n",
    "model_genre_duration = sm.Logit(y, X).fit(method='lbfgs', maxiter=200)\n",
    "print(model_genre_duration.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc426a43-eabf-499c-a1ff-6323ddb8481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_genre_duration.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1cc8-578a-4d51-b0bd-a602c9dae042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients and p-values\n",
    "summary = model_genre_duration.summary2().tables[1]\n",
    "summary = summary.reset_index().rename(columns={\"index\": \"term\", \"Coef.\": \"coef\", \"P>|z|\": \"pval\"})\n",
    "\n",
    "# Exclude intercept\n",
    "summary = summary[summary[\"term\"] != \"const\"]\n",
    "\n",
    "# Top 10 terms by lowest p-value\n",
    "top10 = summary.sort_values(\"pval\", ascending=True).head(15)\n",
    "print(\"Top 15 terms by statistical significance (lowest p-values):\")\n",
    "top10[[\"term\", \"coef\", \"pval\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ca511-aa31-4f19-8b34-c970c8d4d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Venues and raw genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437d745-94b0-42c6-a55d-875c34aa4374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pred = df_venues[df_venues[\"venue_grouped\"].isin(venues_with_reprog)].copy()\n",
    "\n",
    "tbl_genre = pd.crosstab(df_pred[\"genre_merged\"], df_pred[\"reprog\"])\n",
    "tbl_genre.columns = [\"reprog_0\", \"reprog_1\"]\n",
    "\n",
    "mask = (tbl_genre[\"reprog_0\"] >= 5) & (tbl_genre[\"reprog_1\"] >= 1) \n",
    "valid_genres = tbl_genre[mask].index\n",
    "df_pred = df_pred[df_pred[\"genre_merged\"].isin(valid_genres)]\n",
    "\n",
    "# ── 5. Dummy encode genre and venue ─────────────────────────────────────\n",
    "genre_dummies = pd.get_dummies(df_pred[\"genre_merged\"], drop_first=True).astype(float)\n",
    "venue_dummies = pd.get_dummies(df_pred[\"venue_grouped\"], drop_first=True).astype(float)\n",
    "\n",
    "# ── 6. Combine all predictors ───────────────────────────────────────────\n",
    "X = pd.concat([genre_dummies, venue_dummies], axis=1)\n",
    "X = sm.add_constant(X)\n",
    "y = df_pred[\"reprog\"].astype(int)\n",
    "\n",
    "# ── 7. Fit logistic regression ──────────────────────────────────────────\n",
    "model_venues_genre = sm.Logit(y, X).fit(method='lbfgs', maxiter=200)\n",
    "print(model_venues_genre.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da27da-d6c3-4648-945c-279243a744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_venues_genre.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8d166-583c-464a-8533-5d743314b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients and p-values\n",
    "summary = model_venues_genre.summary2().tables[1]\n",
    "summary = summary.reset_index().rename(columns={\"index\": \"term\", \"Coef.\": \"coef\", \"P>|z|\": \"pval\"})\n",
    "\n",
    "# Exclude intercept\n",
    "summary = summary[summary[\"term\"] != \"const\"]\n",
    "\n",
    "# Top 10 terms by lowest p-value\n",
    "top10 = summary.sort_values(\"pval\", ascending=True).head(15)\n",
    "print(\"Top 15 terms by statistical significance (lowest p-values):\")\n",
    "top10[[\"term\", \"coef\", \"pval\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f62dd4-7f2f-42a0-a529-ba0026707f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Venues, raw genre, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34395d78-72ca-49cf-b882-503f68ffe945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ── 1. Filter venue, genre, and duration ───────────────────────────────\n",
    "df_pred = df_venues[df_venues[\"venue_grouped\"].isin(venues_with_reprog)].copy()\n",
    "\n",
    "tbl_genre = pd.crosstab(df_pred[\"genre_merged\"], df_pred[\"reprog\"])\n",
    "tbl_genre.columns = [\"reprog_0\", \"reprog_1\"]\n",
    "valid_genres = tbl_genre[(tbl_genre[\"reprog_0\"] >= 5) & (tbl_genre[\"reprog_1\"] >= 1)].index\n",
    "\n",
    "df_pred = df_pred[df_pred[\"genre_merged\"].isin(valid_genres)]\n",
    "df_pred = df_pred[(df_pred[\"duration_minutes\"] >= 30) & (df_pred[\"duration_minutes\"] <= 105)].copy()\n",
    "\n",
    "# ── 2. Feature creation ────────────────────────────────────────────────\n",
    "spline_duration = dmatrix(\n",
    "    \"bs(duration_minutes, df=4, degree=3, include_intercept=False)\",\n",
    "    data=df_pred,\n",
    "    return_type='dataframe'\n",
    ")\n",
    "\n",
    "genre_dummies = pd.get_dummies(df_pred[\"genre_merged\"], drop_first=True).astype(float)\n",
    "venue_dummies = pd.get_dummies(df_pred[\"venue_grouped\"], drop_first=True).astype(float)\n",
    "\n",
    "# ── 3. Combine all predictors ──────────────────────────────────────────\n",
    "X = pd.concat([spline_duration, genre_dummies, venue_dummies], axis=1)\n",
    "X = sm.add_constant(X)\n",
    "y = df_pred[\"reprog\"].astype(int)\n",
    "\n",
    "# ── 4. Fit logistic regression ─────────────────────────────────────────\n",
    "model_all = sm.Logit(y, X).fit(method='lbfgs', maxiter=200)\n",
    "print(model_all.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810df49a-2f83-46f3-90c1-cab93f8ff34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_all.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760edc2-11db-4551-9ed5-3b9719fa0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients and p-values\n",
    "summary = model_all.summary2().tables[1]\n",
    "summary = summary.reset_index().rename(columns={\"index\": \"term\", \"Coef.\": \"coef\", \"P>|z|\": \"pval\"})\n",
    "\n",
    "# Exclude intercept\n",
    "summary = summary[summary[\"term\"] != \"const\"]\n",
    "\n",
    "# Top 10 terms by lowest p-value\n",
    "top10 = summary.sort_values(\"pval\", ascending=True).head(20)\n",
    "print(\"Top 20 terms by statistical significance (lowest p-values):\")\n",
    "top10[[\"term\", \"coef\", \"pval\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db0ed4-f760-4701-ac47-6301757daa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_predictors = (\n",
    "    model_all.summary2().tables[1]\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"feature\"})\n",
    "    .sort_values(by=\"Coef.\", key=abs, ascending=False))\n",
    "\n",
    "# Filter for statistically significant predictors (p < 0.05)\n",
    "significant_predictors = top_predictors[top_predictors[\"P>|z|\"] < 0.05]\n",
    "\n",
    "# Display top 10 significant predictors\n",
    "significant_predictors.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8407a7-9ab7-42a8-abd2-da735d95f327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599658cb-8b5a-4cd2-b814-c0ecbecb2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vif = X.drop(columns='const', errors='ignore')  # remove constant if present\n",
    "\n",
    "# 2. Calculate VIFs\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "# 3. Sort and display\n",
    "vif_data = vif_data.sort_values(\"VIF\", ascending=False)\n",
    "vif_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ea51e-f572-4ffa-acb3-e7be5d20aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios = np.exp(model_all.params)\n",
    "neg_odds = odds_ratios.sort_values().head(10)\n",
    "print(\"\\n🔻 Top 10 negative odds ratios:\")\n",
    "print(neg_odds)\n",
    "\n",
    "# ── 6. Marginal effects (Average Marginal Effects) ───────────────────\n",
    "mfx = model_all.get_margeff(at=\"overall\", method=\"dydx\")\n",
    "print(\"\\n📉 Marginal Effects:\")\n",
    "print(mfx.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b39b9-ea98-48a4-a72d-d497b91df759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities with confidence intervals\n",
    "pred = model_all.get_prediction(X)\n",
    "summary_frame = pred.summary_frame(alpha=0.05)\n",
    "\n",
    "df_pred[\"pred_prob\"] = summary_frame[\"predicted\"]\n",
    "df_pred[\"ci_lower\"] = summary_frame[\"ci_lower\"]\n",
    "df_pred[\"ci_upper\"] = summary_frame[\"ci_upper\"]\n",
    "\n",
    "# Aggregate by venue\n",
    "venue_avg = (\n",
    "    df_pred.groupby(\"venue_grouped\")[[\"pred_prob\", \"ci_lower\", \"ci_upper\"]]\n",
    "    .mean()\n",
    "    .sort_values(\"pred_prob\", ascending=True)\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, len(venue_avg) * 0.3))\n",
    "plt.errorbar(\n",
    "    venue_avg[\"pred_prob\"],\n",
    "    venue_avg.index,\n",
    "    xerr=[\n",
    "        venue_avg[\"pred_prob\"] - venue_avg[\"ci_lower\"],\n",
    "        venue_avg[\"ci_upper\"] - venue_avg[\"pred_prob\"]\n",
    "    ],\n",
    "    fmt='o',\n",
    "    color=\"darkred\",\n",
    "    ecolor=\"orange\",\n",
    "    capsize=3\n",
    ")\n",
    "plt.xlabel(\"Predicted Probability of Reprogramming\")\n",
    "plt.ylabel(\"Venue\")\n",
    "plt.title(\"Predicted Reprogramming Probabilities by Venue\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380c4cf-753f-4d4e-aed5-ba409d5610b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Predict probabilities with confidence intervals ──────────────\n",
    "pred = model_all.get_prediction(X)\n",
    "summary_frame = pred.summary_frame()\n",
    "\n",
    "df_pred[\"pred_prob\"] = summary_frame[\"predicted\"]\n",
    "df_pred[\"ci_lower\"] = summary_frame[\"ci_lower\"]\n",
    "df_pred[\"ci_upper\"] = summary_frame[\"ci_upper\"]\n",
    "\n",
    "# ── 2. Aggregate by venue ───────────────────────────────────────────\n",
    "venue_avg = (\n",
    "    df_pred.groupby(\"venue_grouped\")[[\"pred_prob\", \"ci_lower\", \"ci_upper\"]]\n",
    "    .mean()\n",
    "    .sort_values(\"pred_prob\", ascending=True)\n",
    "    .tail(15)  # ⬅️ Top 15 venues only\n",
    ")\n",
    "\n",
    "# ── 3. Plot with error bars ─────────────────────────────────────────\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.errorbar(\n",
    "    venue_avg[\"pred_prob\"],\n",
    "    venue_avg.index,\n",
    "    xerr=[\n",
    "        venue_avg[\"pred_prob\"] - venue_avg[\"ci_lower\"],\n",
    "        venue_avg[\"ci_upper\"] - venue_avg[\"pred_prob\"]\n",
    "    ],\n",
    "    fmt='o',\n",
    "    color=\"darkred\",\n",
    "    ecolor=\"orange\",\n",
    "    capsize=3,\n",
    "    markersize=5,\n",
    "    elinewidth=1.2\n",
    ")\n",
    "\n",
    "# ── 4. Customize appearance ─────────────────────────────────────────\n",
    "plt.xlabel(\"Predicted Probability of Reprogramming\")\n",
    "plt.ylabel(\"Venue\")\n",
    "plt.title(\"Top 15 Venues by Predicted Reprogramming Probability (95% CI)\")\n",
    "plt.tick_params(axis='both', labelsize=8)\n",
    "plt.grid(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ── 5. Save the figure ──────────────────────────────────────────────\n",
    "plt.savefig(os.path.join(FIG_PATH, \"predicted_venues_top15.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(os.path.join(FIG_PATH, \"predicted_venues_top15.pdf\"), bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b58b0-4c3f-49f0-abbe-f5c86aeb057c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb935f4-5a20-450b-a3bc-f16b1d1ed32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_pred, x=\"duration_minutes\", y=\"pred_prob\", hue=\"genre_merged\", alpha=0.5)\n",
    "plt.title(\"Predicted Probability vs Duration (colored by Genre)\")\n",
    "plt.ylabel(\"Predicted Probability of Reprogramming\")\n",
    "plt.xlabel(\"Duration (minutes)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7a3a0-cbe7-4dba-86ea-ca977a32b611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95388e78-0293-4bed-9ef9-4b59984d84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create and fit pipeline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "ridge_pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=False),\n",
    "    LogisticRegressionCV(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"liblinear\",\n",
    "        cv=3,\n",
    "        scoring=\"neg_log_loss\",\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# ✅ Fit the model\n",
    "ridge_pipeline.fit(X, y)  # make sure X is a DataFrame and y is your target\n",
    "\n",
    "# 2. Access the model\n",
    "ridge_model = ridge_pipeline.named_steps[\"logisticregressioncv\"]\n",
    "\n",
    "# ✅ Ensure model is fitted\n",
    "assert hasattr(ridge_model, \"coef_\"), \"Model must be fitted before accessing .coef_\"\n",
    "\n",
    "# 3. Extract and inspect coefficients\n",
    "feature_names = X.columns\n",
    "coefs = pd.Series(ridge_model.coef_[0], index=feature_names)\n",
    "\n",
    "coefs_sorted = coefs.sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"🔝 Top predictors (Ridge):\")\n",
    "print(coefs_sorted.head(10))\n",
    "\n",
    "print(\"\\n🔻 Least influential (Ridge):\")\n",
    "print(coefs_sorted.tail(10))\n",
    "\n",
    "scoring = {\n",
    "    \"neg_log_loss\": \"neg_log_loss\",\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"roc_auc\": \"roc_auc\"\n",
    "}\n",
    "\n",
    "results = cross_validate(\n",
    "    ridge_pipeline, X, y,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Cross-validated performance (Ridge, full model):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric}: {values.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b706d-7da2-414e-a8a9-93b84e134720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipeline.fit(X, y)\n",
    "best_C = ridge_pipeline.named_steps['logisticregressioncv'].C_[0]\n",
    "print(f\"\\nBest regularization (C): {best_C:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020e7f6-b7c8-4acc-98ce-85bda19f4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy model that always predicts the majority class\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X, y)\n",
    "\n",
    "# Predict on the same set (or cross-validate)\n",
    "y_pred = dummy.predict(X)\n",
    "y_proba = dummy.predict_proba(X)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Baseline Accuracy:\", (y_pred == y).mean())\n",
    "print(\"Baseline AUC:\", roc_auc_score(y, y_proba))\n",
    "print(\"Baseline Log loss:\", log_loss(y, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a135be-654e-4b63-8a4c-66b198fef855",
   "metadata": {},
   "source": [
    "#### Test XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb483e4e-5491-4236-832c-2b59cf1dbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_duration = dmatrix(\n",
    "    \"bs(duration_minutes, df=4, degree=3, include_intercept=False)\",\n",
    "    data=df_pred,\n",
    "    return_type=\"dataframe\"\n",
    ")\n",
    "spline_duration.columns = [f\"spline_{i}\" for i in range(spline_duration.shape[1])]\n",
    "\n",
    "# 2. Combine all features\n",
    "X_final = pd.concat([spline_duration, genre_dummies, venue_dummies], axis=1).astype(float)\n",
    "y_final = df_pred[\"reprog\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d2abe-3c53-464e-b26a-d714fbba3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = 2.1 # based on class imbalance (adjust if needed)\n",
    "\n",
    "# ── 1. Fit XGBoost on filtered data ───────────────────────────────\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=4\n",
    ")\n",
    "model.fit(X_final, y_final)\n",
    "\n",
    "# ── 2. Predict and evaluate ────────────────────────────────────────\n",
    "y_pred = model.predict(X_final)\n",
    "\n",
    "accuracy = accuracy_score(y_final, y_pred)\n",
    "recall_macro = recall_score(y_final, y_pred, average=\"macro\")\n",
    "f1_macro = f1_score(y_final, y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro Recall: {recall_macro:.4f}\")\n",
    "print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_final, y_pred, target_names=[\"Not Reprog\", \"Reprog\"]))\n",
    "\n",
    "# ── 3. Confusion matrix ────────────────────────────────────────────\n",
    "cm = confusion_matrix(y_final, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Reprog\", \"Reprog\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix (XGBoost, Filtered Data)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b2861-ab7f-475b-a2f0-8c3efd3266ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"neg_log_loss\": \"neg_log_loss\"\n",
    "}\n",
    "\n",
    "# ── 2. Run cross-validation ───────────────────────────────────────────\n",
    "results = cross_validate(model, X_final, y_final, cv=cv, scoring=scoring, return_estimator=True)\n",
    "\n",
    "print(\"\\nXGBoost Cross-Validated Performance (scale_pos_weight=2.0):\")\n",
    "for metric in scoring:\n",
    "    mean_score = results[f'test_{metric}'].mean()\n",
    "    print(f\"{metric}: {mean_score:.4f}\")\n",
    "\n",
    "# ── 3. Feature importance from best estimator ─────────────────────────\n",
    "# Use the estimator with best ROC AUC\n",
    "best_idx = np.argmax(results['test_roc_auc'])\n",
    "best_model = results['estimator'][best_idx]\n",
    "\n",
    "# Plot feature importances\n",
    "importances = best_model.feature_importances_\n",
    "feature_names = X_final.columns\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0caef-c79a-4502-a6c2-0a87d9a998c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. Define and register custom colormap\n",
    "matplotlib_cmap = LinearSegmentedColormap.from_list(\n",
    "    \"custom_orange_yellow_red\", [\"yellow\", \"orange\", \"darkred\"]\n",
    ")\n",
    "\n",
    "# ✅ 2. Compute SHAP values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_final)  # NOT .shap_values()\n",
    "\n",
    "# ✅ 3. Get top 15 features by mean absolute SHAP value\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X_final.columns,\n",
    "    'mean_abs_shap': np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "top15_features = shap_importance['feature'].head(15).tolist()\n",
    "X_top15 = X_final[top15_features]\n",
    "shap_values_top15 = shap_values[:, [X_final.columns.get_loc(f) for f in top15_features]]\n",
    "\n",
    "# ✅ 4. Plot SHAP summary for top 15 with custom styling\n",
    "shap.summary_plot(\n",
    "    shap_values_top15,\n",
    "    X_top15,\n",
    "    plot_type=\"dot\",\n",
    "    cmap=matplotlib_cmap,\n",
    "    axis_color=\"#000000\",\n",
    "    show=False,\n",
    "    plot_size=(6, 5)\n",
    ")\n",
    "\n",
    "# ✅ 5. Customize labels and appearance\n",
    "plt.xlabel(\"Impact on Reprogr. Pred.\", fontsize=9)\n",
    "plt.ylabel(\"Feature\", fontsize=9)\n",
    "plt.tick_params(labelsize=7)\n",
    "plt.title(\"Top 15 Predictors (SHAP)\", fontsize=9)\n",
    "\n",
    "# ✅ 6. Save\n",
    "fig = plt.gcf()\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(FIG_PATH, \"shap_summary2.png\"), dpi=300, bbox_inches='tight')\n",
    "fig.savefig(os.path.join(FIG_PATH, \"shap_summary2.pdf\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80eabd4-eef6-45e1-9f7f-30044580cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_final, y_final, stratify=y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Define model\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=4\n",
    ")\n",
    "\n",
    "# 3. Set eval_metric if .fit() doesn't accept it directly\n",
    "model.set_params(eval_metric='logloss')\n",
    "\n",
    "# 4. Fit with validation set\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 5. Extract learning curve results\n",
    "results = model.evals_result()\n",
    "\n",
    "# 6. Plot learning curve\n",
    "epochs = len(results['validation_0']['logloss'])\n",
    "x_axis = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "plt.plot(x_axis, results['validation_1']['logloss'], label='Validation')\n",
    "plt.xlabel(\"Boosting Round\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"XGBoost Learning Curve (Log Loss)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2d7c7-c982-4c86-a619-598f2cdf205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_final, y_proba)\n",
    "ap = average_precision_score(y_final, y_proba)\n",
    "\n",
    "plt.plot(recall, precision, label=f\"AP = {ap:.2f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c0430-3b05-4e96-a34f-fe751f64adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_final, y_proba, n_bins=10)\n",
    "plt.plot(prob_pred, prob_true, marker='o', label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Perfectly Calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475fe78-dabc-4d90-913b-f807f107c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_proba = model.predict_proba(X_final)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_final, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63943a3-84cf-46ad-82ee-70f70fc95812",
   "metadata": {},
   "source": [
    "#### Time Slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d1a50-1e77-4daa-8ca5-082bc54b0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str_to_hour(t):\n",
    "    \"\"\"Convert '11h15' or '15h' to fractional hour (e.g., 11.25 or 15.0).\"\"\"\n",
    "    if pd.isna(t):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Remove whitespace and standardize format\n",
    "        t = str(t).strip().lower()\n",
    "        if \"h\" not in t:\n",
    "            return np.nan\n",
    "        hour_min = t.split(\"h\")\n",
    "        hour = int(hour_min[0])\n",
    "        minute = int(hour_min[1]) if len(hour_min) > 1 and hour_min[1].isdigit() else 0\n",
    "        return hour + minute / 60\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "df[\"start_hour\"] = df[\"start_time\"].apply(time_str_to_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ce58a-8c7b-488d-81ba-4a6be67768fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"start_hour\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1839e6-a730-45d1-a705-5169028ba295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep shows that start between 8 AM and midnight\n",
    "df_time = df[(df[\"start_hour\"] >= 9) & (df[\"start_hour\"] <= 24)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a581979-4d16-4c8f-b605-e5ab4303ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_time = dmatrix(\"bs(start_hour, df=4, degree=3, include_intercept=False)\",\n",
    "                      data=df_time, return_type='dataframe')\n",
    "\n",
    "X = pd.concat([spline_time], axis=1)\n",
    "X = sm.add_constant(X)\n",
    "y = df_time[\"reprog\"].astype(int)\n",
    "\n",
    "model_time = sm.Logit(y, X).fit()\n",
    "print(model_time.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78286357-476a-44a8-a614-f06f923b384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIC:\", model_time.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5180eb3-90b4-4f06-ac1a-d48d23d15990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Smooth hour grid ────────────────────────────────────────────────\n",
    "hour_grid = np.linspace(6, 23, 200)            # adjust min/max if needed\n",
    "\n",
    "# ── 2. Spline basis for the grid ───────────────────────────────────────\n",
    "spline_grid = dmatrix(\n",
    "    \"bs(hour_grid, df=4, degree=3, include_intercept=False)\",\n",
    "    {\"hour_grid\": hour_grid},\n",
    "    return_type=\"dataframe\"\n",
    ")\n",
    "\n",
    "X_grid = sm.add_constant(spline_grid)\n",
    "logit_pred = model_time.predict(X_grid, which=\"linear\")\n",
    "\n",
    "# ── 3. Delta-method 95 % CI on the probability scale ───────────────────\n",
    "cov = model_time.cov_params()\n",
    "# Ensure design matrix is NumPy array\n",
    "X_grid_np = X_grid.values  # convert to NumPy array\n",
    "cov_np = model_time.cov_params().values  # also convert covariance to NumPy\n",
    "\n",
    "# Compute standard errors on the logit scale\n",
    "se = np.sqrt(np.einsum(\"ij,jk,ik->i\", X_grid_np, cov_np, X_grid_np))\n",
    "\n",
    "logit_low  = logit_pred - 1.96 * se\n",
    "logit_high = logit_pred + 1.96 * se\n",
    "prob_pred  = 1 / (1 + np.exp(-logit_pred))\n",
    "prob_low   = 1 / (1 + np.exp(-logit_low))\n",
    "prob_high  = 1 / (1 + np.exp(-logit_high))\n",
    "\n",
    "# ── 4. Plot with orange styling & black axes ───────────────────────────\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(hour_grid, prob_pred, color=\"#e6550d\", linewidth=2)\n",
    "plt.fill_between(hour_grid, prob_low, prob_high, color=\"#fdae6b\", alpha=0.35)\n",
    "\n",
    "# Axis labels & aesthetics\n",
    "plt.xlabel(\"Start Hour of Performance\", color=\"black\")\n",
    "plt.ylabel(\"Predicted P(reprog = 1)\", color=\"black\")\n",
    "plt.title(\"Spline Fit: Start Time → Reprogramming Probability\", color=\"black\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "ax = plt.gca()\n",
    "for spine in [\"left\", \"bottom\"]:\n",
    "    ax.spines[spine].set_color(\"black\")\n",
    "    ax.spines[spine].set_linewidth(1.2)\n",
    "for spine in [\"top\", \"right\"]:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "ax.tick_params(axis=\"x\", colors=\"black\")\n",
    "ax.tick_params(axis=\"y\", colors=\"black\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefff4ea-d5b7-43c7-a4ba-b4c2c3500892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs from both columns\n",
    "df_valid = df[[\"start_hour\", \"reprog\"]].dropna()\n",
    "\n",
    "# Bin start_hour\n",
    "bins = pd.cut(df_valid[\"start_hour\"], bins=range(0, 25), right=False)\n",
    "\n",
    "# Compute Mutual Information\n",
    "mi = mutual_info_score(bins, df_valid[\"reprog\"])\n",
    "print(f\"Mutual Information (start_hour vs reprog): {mi:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc257-f88d-441c-8151-70033027727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[[\"start_hour\", \"reprog\"]].dropna()\n",
    "\n",
    "# Pearson correlation (equivalent to point-biserial when one var is binary)\n",
    "r, p = stats.pearsonr(df_corr[\"start_hour\"], df_corr[\"reprog\"])\n",
    "\n",
    "# Display\n",
    "print(f\"Filtered rows used: {len(df_corr)}\")\n",
    "print(f\"Point-biserial correlation r  = {r:.3f}\")\n",
    "print(f\"p-value                        = {p:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0cf51-9324-412d-83f1-274e04710756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[[\"start_hour\", \"reprog\"]].dropna()\n",
    "\n",
    "# Define feature and target\n",
    "X = df_corr[[\"start_hour\"]]  # must be 2D\n",
    "y = df_corr[\"reprog\"]\n",
    "\n",
    "# Fit simple logistic model using only this feature\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict probabilities\n",
    "y_scores = clf.predict_proba(X)[:, 1]\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(y, y_scores)\n",
    "\n",
    "# Display\n",
    "print(f\"Filtered rows used: {len(df_corr)}\")\n",
    "print(f\"AUC (1-feature model): {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c828d23-b428-4c18-9db1-d5696bfa633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import unicodedata\n",
    "\n",
    "# 1. Extract all unique institution names\n",
    "unique_insts = exploded[\"institutions\"].dropna().unique().tolist()\n",
    "\n",
    "# 2. Normalize: lowercase and strip accents\n",
    "def normalize(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = text.lower().strip()\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "normalized_map = {inst: normalize(inst) for inst in unique_insts}\n",
    "\n",
    "# 3. Group similar names based on fuzzy score\n",
    "threshold = 80  # adjust as needed (90 = very close match)\n",
    "canonical_names = {}\n",
    "used = set()\n",
    "\n",
    "for inst, norm_inst in normalized_map.items():\n",
    "    if inst in used:\n",
    "        continue\n",
    "    # Find close matches\n",
    "    matches = process.extract(\n",
    "        norm_inst,\n",
    "        [normalize(i) for i in unique_insts],\n",
    "        scorer=fuzz.token_sort_ratio,\n",
    "        score_cutoff=threshold,\n",
    "        limit=None  # return all above threshold\n",
    "    )\n",
    "    group = [match[0] for match in matches]\n",
    "    for match in group:\n",
    "        # Map all matches to this canonical name\n",
    "        original = [k for k, v in normalized_map.items() if v == match]\n",
    "        for orig in original:\n",
    "            canonical_names[orig] = inst\n",
    "            used.add(orig)\n",
    "\n",
    "# 4. Apply to exploded DataFrame\n",
    "exploded[\"institution_grouped\"] = exploded[\"institutions\"].map(canonical_names).fillna(exploded[\"institutions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efea9b2-824d-4b59-bfe3-116e1d7f99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded[\"institution_grouped\"].nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f922a5-1ef8-4980-96cf-918c79932163",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded[\"institutions\"].nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d9f23-bb04-4872-a2c3-929ffa62be29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_stats = (\n",
    "    exploded.groupby(\"institution_grouped\")[\"reprog\"]\n",
    "            .agg(total_shows=\"count\", reprog_count=\"sum\")\n",
    "            .assign(percent_reprog=lambda d: (100 * d[\"reprog_count\"] / d[\"total_shows\"]).round(1))\n",
    "            .sort_values(\"total_shows\", ascending=False)\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "display(grouped_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc30c5d-3095-4b38-b465-5917c91400d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_grp = exploded.dropna(subset=[\"institution_grouped\", \"reprog\"])\n",
    "\n",
    "exploded_grp = exploded_grp.assign(flag=1).reset_index(drop=True)\n",
    "\n",
    "dummy_grp = exploded_grp.pivot_table(\n",
    "    index=exploded_grp.index,\n",
    "    columns=\"institution_grouped\",\n",
    "    values=\"flag\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# attach reprog\n",
    "dummy_grp[\"reprog\"] = exploded_grp[\"reprog\"].astype(int)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Chi-square & Cramér’s V for each grouped institution\n",
    "# ------------------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for col in dummy_grp.columns.drop(\"reprog\"):\n",
    "    if dummy_grp[col].sum() < 3:          # keep only institutions in ≥5 shows\n",
    "        continue\n",
    "    ct = pd.crosstab(dummy_grp[col], dummy_grp[\"reprog\"])\n",
    "    if ct.shape != (2, 2):\n",
    "        continue                         # need both 0/1 rows and columns\n",
    "    chi2, p, *_ = chi2_contingency(ct)\n",
    "    n  = ct.to_numpy().sum()\n",
    "    V  = np.sqrt(chi2 / n)               # k-1 = 1 for a 2×2 table\n",
    "    results.append((col, dummy_grp[col].sum(), V, p))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Assemble results and display\n",
    "# ------------------------------------------------------------------\n",
    "res_df = (pd.DataFrame(results,\n",
    "                       columns=[\"institution_grouped\",\n",
    "                                \"shows_with_inst\",\n",
    "                                \"cramers_v\",\n",
    "                                \"p_value\"])\n",
    "          .sort_values(\"cramers_v\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)   # show all if desired\n",
    "print(\"🏆 Grouped institutions ranked by association strength with reprog:\")\n",
    "display(res_df.head(25))                  # top 25 (adjust as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2f07a-ede0-49bb-b939-904f74e02d58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Explode the 'institutions' list\n",
    "exploded = df[df[\"institutions\"].str.len() > 0].explode(\"institutions\")\n",
    "\n",
    "# 2. Drop rows with missing values (safety)\n",
    "exploded = exploded.dropna(subset=[\"institutions\", \"reprog\"])\n",
    "\n",
    "# 3. Group and calculate stats\n",
    "inst_stats = (\n",
    "    exploded.groupby(\"institutions\")[\"reprog\"]\n",
    "            .agg(total_shows=\"count\", reprog_count=\"sum\")\n",
    "            .assign(percent_reprog=lambda d: (100 * d[\"reprog_count\"] / d[\"total_shows\"]).round(1))\n",
    "            .sort_values(\"total_shows\", ascending=False)\n",
    ")\n",
    "\n",
    "# 4. Ensure full display (no truncation)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# 5. Display result\n",
    "print(\"📊 Reprogramming percentage per institution (all listed):\")\n",
    "display(inst_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f64d5-0939-4e1c-8c80-5c1210410a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exploded[\"institutions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f909b1-1504-490e-a8b4-c6219dc63ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Clean and group\n",
    "df[\"n_actors\"] = pd.to_numeric(df[\"n_actors\"], errors=\"coerce\")\n",
    "df_filtered = df.dropna(subset=[\"n_actors\", \"reprog\"])\n",
    "\n",
    "# 2. Count total and reprogrammed\n",
    "actor_counts = (\n",
    "    df_filtered.groupby(\"n_actors\")[\"reprog\"]\n",
    "    .agg(total=\"count\", reprog=\"sum\")\n",
    "    .reset_index()\n",
    "    .astype({\"n_actors\": int})\n",
    ")\n",
    "\n",
    "# 3. Compute non-reprogrammed counts\n",
    "actor_counts[\"non_reprog\"] = actor_counts[\"total\"] - actor_counts[\"reprog\"]\n",
    "\n",
    "# 4. Plot stacked bars\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = actor_counts[\"n_actors\"]\n",
    "\n",
    "plt.bar(x, actor_counts[\"non_reprog\"], label=\"Non-Reprogrammed\", color=\"lightgray\")\n",
    "plt.bar(x, actor_counts[\"reprog\"], bottom=actor_counts[\"non_reprog\"], label=\"Reprogrammed\", color=\"tomato\")\n",
    "\n",
    "# 5. Style\n",
    "plt.xlabel(\"Number of Actors\")\n",
    "plt.ylabel(\"Number of Shows\")\n",
    "plt.title(\"🎭 Stacked Distribution of Shows by Number of Actors and Reprog Status\")\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c79396-c0e7-4544-b739-d1cbe4d7c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Clean and group\n",
    "df[\"n_actors\"] = pd.to_numeric(df[\"n_actors\"], errors=\"coerce\")\n",
    "df_filtered = df.dropna(subset=[\"n_actors\", \"reprog\"])\n",
    "\n",
    "# 2. Count total shows and reprogrammed shows\n",
    "actor_counts = (\n",
    "    df_filtered.groupby(\"n_actors\")[\"reprog\"]\n",
    "    .agg(total=\"count\", reprog=\"sum\")\n",
    "    .reset_index()\n",
    "    .astype({\"n_actors\": int})\n",
    ")\n",
    "\n",
    "# 3. Plot grouped bars\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.4\n",
    "x = actor_counts[\"n_actors\"]\n",
    "\n",
    "plt.bar(x - bar_width/2, actor_counts[\"total\"], width=bar_width, label=\"Total shows\", color=\"lightgray\")\n",
    "plt.bar(x + bar_width/2, actor_counts[\"reprog\"], width=bar_width, label=\"Reprogrammed shows\", color=\"tomato\")\n",
    "\n",
    "# 4. Style\n",
    "plt.xlabel(\"Number of Actors\")\n",
    "plt.ylabel(\"Number of Shows\")\n",
    "plt.title(\"🎭 Total and Reprogrammed Shows by Number of Actors\")\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a936a9e-6772-46d3-9dea-e0dea8651c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Clean data\n",
    "df_viz = df.dropna(subset=[\"genre_clean\", \"reprog\"])\n",
    "\n",
    "# 2. Create counts per genre and reprog status\n",
    "genre_counts = (\n",
    "    df_viz.groupby([\"genre_clean\", \"reprog\"])\n",
    "    .size()\n",
    "    .unstack(level=1, fill_value=0)\n",
    ")\n",
    "\n",
    "# 3. Ensure both columns 0 and 1 exist\n",
    "for col in [0, 1]:\n",
    "    if col not in genre_counts.columns:\n",
    "        genre_counts[col] = 0\n",
    "\n",
    "# 4. Normalize to percentages\n",
    "genre_props = genre_counts.div(genre_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# 5. Sort by reprog = 1\n",
    "genre_props = genre_props.sort_values(by=1, ascending=False)\n",
    "\n",
    "# 6. Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "genre_props[[0, 1]].plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    color=[\"lightgray\", \"tomato\"],\n",
    "    figsize=(10, 6),\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# 7. Style\n",
    "plt.title(\"🎭 Percentage of Reprogrammed Shows by Genre\", fontsize=14)\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend([\"Not Reprogrammed\", \"Reprogrammed\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc04196-c7dc-40b2-afd5-331265142c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80b709-d819-447a-ad05-e8e7f70db312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Filter relevant data\n",
    "df_viz = df.dropna(subset=[\"festival_theater\", \"reprog\"])\n",
    "\n",
    "# 2. Create counts per venue and reprog status\n",
    "venue_counts = (\n",
    "    df_viz.groupby([\"festival_theater\", \"reprog\"])\n",
    "    .size()\n",
    "    .unstack(level=1, fill_value=0)  # Make sure both 0 and 1 are present\n",
    ")\n",
    "\n",
    "# 3. Ensure both columns exist\n",
    "for col in [0, 1]:\n",
    "    if col not in venue_counts.columns:\n",
    "        venue_counts[col] = 0\n",
    "\n",
    "# 4. Normalize row-wise to percentages\n",
    "venue_props = venue_counts.div(venue_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# 5. Sort by proportion of reprog = 1\n",
    "venue_props = venue_props.sort_values(by=1, ascending=False)\n",
    "\n",
    "# 6. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "venue_props[[0, 1]].plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    color=[\"lightgray\", \"tomato\"],\n",
    "    figsize=(12, 6),\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# 7. Style\n",
    "plt.title(\"🎭 Percentage of Reprogrammed Shows by Venue\", fontsize=14)\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Venue (festival_theater)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend([\"Not Reprogrammed\", \"Reprogrammed\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850c13a-d5e4-4845-be2f-78eceec4dd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49a7fb-bf5e-43cb-b1bd-0974454d9a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc1294-d812-4891-a7bd-03ecfa26d0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be230dab-11ef-4d41-a197-c7b06550ee17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09424c80-9207-4c40-a742-608568ecf1c7",
   "metadata": {},
   "source": [
    "### Inter-corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba984fd1-47d0-46eb-8e91-2bfa568b178b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec51d5e-6441-4ae8-948c-bd352dffd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter missing values\n",
    "df_viz = df.dropna(subset=[\"genre_clean\", \"festival_theater\"])\n",
    "\n",
    "# 2. Create contingency table\n",
    "contingency = pd.crosstab(df_viz[\"genre_clean\"], df_viz[\"festival_theater\"])\n",
    "\n",
    "# 3. Run chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "# 4. Compute Cramér’s V\n",
    "n = contingency.sum().sum()\n",
    "min_dim = min(contingency.shape) - 1\n",
    "cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "\n",
    "# 5. Print results\n",
    "print(\"📊 Chi-squared test for independence\")\n",
    "print(f\"Chi² statistic = {chi2:.3f}\")\n",
    "print(f\"p-value        = {p:.4g}\")\n",
    "print(f\"Cramér’s V     = {cramers_v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0e34a-2d17-4a79-90c7-ef016e392dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter missing values\n",
    "df_viz = df.dropna(subset=[\"genre\", \"festival_theater\"])\n",
    "\n",
    "# 2. Create contingency table\n",
    "contingency = pd.crosstab(df_viz[\"genre\"], df_viz[\"festival_theater\"])\n",
    "\n",
    "# 3. Run chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "# 4. Compute Cramér’s V\n",
    "n = contingency.sum().sum()\n",
    "min_dim = min(contingency.shape) - 1\n",
    "cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "\n",
    "# 5. Print results\n",
    "print(\"📊 Chi-squared test for independence\")\n",
    "print(f\"Chi² statistic = {chi2:.3f}\")\n",
    "print(f\"p-value        = {p:.4g}\")\n",
    "print(f\"Cramér’s V     = {cramers_v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24ffa3-2338-4abe-a3d5-9935f817463e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30932b-066e-42a8-83c3-74425da7ed22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa8fa1-a0db-4c6f-8bcf-6a27ffda5fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd51c1f-61b9-4021-8fe0-f3c12836297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1cb8d-0e70-4510-ba3e-be48f8d33bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# 1. Contingency table\n",
    "contingency = pd.crosstab(df[\"festival_theater\"], df[\"reprog\"])\n",
    "\n",
    "# 2. Chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "# 3. Cramér’s V\n",
    "n = contingency.sum().sum()\n",
    "min_dim = min(contingency.shape) - 1\n",
    "cramers_v = np.sqrt(chi2 / (n * min_dim))\n",
    "\n",
    "# 4. Results\n",
    "print(\"📊 Chi-squared test for independence\")\n",
    "print(f\"Chi2 statistic = {chi2:.3f}\")\n",
    "print(f\"p-value        = {p:.4f}\")\n",
    "print(f\"Cramér’s V     = {cramers_v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72adaf7-013b-4d51-b783-338e8c08cd8d",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a08509-bf8e-40a1-99b8-e9ba73094f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32758f-54da-4134-9542-3a4f527c8bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93df29-c0e7-441f-af18-e18b2301d782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a17e50-17d5-4308-93be-e5f586c2b690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d67d0f-6e31-40d2-b8f5-98098abfb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, RocCurveDisplay\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1️⃣  Train / test split  (80 % train, 20 % test, stratified)\n",
    "# ---------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2️⃣  Fit Logistic Regression\n",
    "#    * class_weight=\"balanced\" compensates for class-imbalance\n",
    "#    * max_iter large enough to converge\n",
    "# ---------------------------------------------------------------\n",
    "clf = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=2000,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3️⃣  Evaluate on the hold-out test set\n",
    "# ---------------------------------------------------------------\n",
    "y_pred   = clf.predict(X_test)\n",
    "y_proba  = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n📊 Classification report (test set):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Not-prog (0)\", \"Prog (1)\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix (test)\")\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC (optional)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC: {auc:.3f}\")\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4️⃣  Feature importance  →  odds-ratios\n",
    "# ---------------------------------------------------------------\n",
    "odds_ratios = pd.Series(\n",
    "    np.exp(clf.coef_[0]),\n",
    "    index=X.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n🔥 Top positive effects (OR > 1):\")\n",
    "display(odds_ratios.head(20).round(3))\n",
    "\n",
    "print(\"\\n❄️ Top negative effects (OR < 1):\")\n",
    "display(odds_ratios.tail(20).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa76017-b94b-46d4-9780-99564df104b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Clean base data\n",
    "df_clean = df.dropna(subset=[\n",
    "    \"duration_minutes\", \"n_actors\", \"genre_clean\", \"festival_theater\", \"reprog\"\n",
    "]).copy()\n",
    "\n",
    "# Remove known outliers\n",
    "df_clean = df_clean[~df_clean[\"duration_minutes\"].isin([1440, 3300])]\n",
    "df_clean[\"n_actors\"] = pd.to_numeric(df_clean[\"n_actors\"], errors=\"coerce\")\n",
    "df_clean[\"duration_minutes\"] = pd.to_numeric(df_clean[\"duration_minutes\"], errors=\"coerce\")\n",
    "df_clean = df_clean.dropna(subset=[\"n_actors\", \"duration_minutes\"])\n",
    "\n",
    "# --- 2. Create dummies\n",
    "X = pd.get_dummies(\n",
    "    df_clean[[\"n_actors\", \"duration_minutes\", \"genre_clean\", \"festival_theater\"]],\n",
    "    drop_first=True\n",
    ").astype(float)\n",
    "\n",
    "# --- 3. Remove columns with no variance or extreme sparsity\n",
    "X = X.loc[:, X.nunique() > 1]\n",
    "X = X.loc[:, X.sum() > 2]\n",
    "\n",
    "# --- 4. Add constant manually\n",
    "X = add_constant(X, has_constant='add')\n",
    "y = df_clean[\"reprog\"].astype(int)\n",
    "\n",
    "# --- 5. Remove multicollinear columns using VIF (variance inflation factor)\n",
    "def drop_high_vif(X, threshold=100):\n",
    "    dropped_cols = []\n",
    "    while True:\n",
    "        vif = pd.Series(\n",
    "            [variance_inflation_factor(X.values, i) for i in range(X.shape[1])],\n",
    "            index=X.columns\n",
    "        )\n",
    "        max_vif = vif.drop(\"const\").max()\n",
    "        if max_vif > threshold:\n",
    "            col_to_drop = vif.drop(\"const\").idxmax()\n",
    "            dropped_cols.append(col_to_drop)\n",
    "            X = X.drop(columns=[col_to_drop])\n",
    "        else:\n",
    "            break\n",
    "    return X, dropped_cols\n",
    "\n",
    "X, dropped = drop_high_vif(X)\n",
    "print(\"Dropped due to high VIF:\", dropped)\n",
    "\n",
    "# --- 6. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- 7. Fit Probit model\n",
    "probit_model = sm.Probit(y_train, X_train)\n",
    "probit_result = probit_model.fit()\n",
    "\n",
    "# --- 8. Predict + evaluate\n",
    "y_proba = probit_result.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "print(f\"\\n📊 Classification report (threshold = {threshold:.2f}):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9a0e0-ee5f-4560-8b11-f67d4911b3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01044e-832c-4d2e-a398-100adf08fb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bedc1-a974-4a1f-8a05-8cecf6307795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6639e8-1cdf-4e8a-a33a-546062ab81f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb879f5-1959-469e-a61e-3f32e8808523",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224350c0-5492-4e6b-b736-b7665f7d7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/Users/antonioslagarias/Documents/OFF/Exports/adho/data_adho_prog.xlsx\"  \n",
    "data_draft=pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312da0c4-40f8-45d0-b176-0d368223fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777c2e1-57cd-47b1-9abe-cf58a9cd18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)  # Show all rows in output\n",
    "print(data_draft[\"festival_theater\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7464e-e55a-451d-aa90-a7efa5f77564",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4dae80-e333-4acb-ab6f-40698a09f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_draft[[\"reprog\", \"n_actors\", \"duration_minutes\",\n",
    "                 \"genre\", \"festival_theater\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232c506-830f-4641-837c-9f4d9350fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_actors\"] = pd.to_numeric(df[\"n_actors\"], errors=\"coerce\")\n",
    "df[\"duration_minutes\"] = pd.to_numeric(df[\"duration_minutes\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe461a0-d3de-4e03-82d2-3e6f6a80bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['festival_theater'] = df['festival_theater'].fillna(\"Inconnu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5317b5-2f0e-49a3-97fb-94d0bd505648",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_map = {\n",
    "    # Théâtre / Café-théâtre\n",
    "    \"théâtre\": \"Théâtre / Café-théâtre\", \"café-théâtre\": \"Théâtre / Café-théâtre\",\n",
    "    \"comédie\": \"Théâtre / Café-théâtre\", \"drame\": \"Théâtre / Café-théâtre\",\n",
    "    \"boulevard\": \"Théâtre / Café-théâtre\", \"humour\": \"Théâtre / Café-théâtre\",\n",
    "    \"théâtre sonore\": \"Théâtre / Café-théâtre\",\n",
    "\n",
    "    # Spectacle musical / Concert\n",
    "    \"théâtre musical\": \"Spectacle musical / Concert\", \"spectacle musical\": \"Spectacle musical / Concert\",\n",
    "    \"concert\": \"Spectacle musical / Concert\", \"classique\": \"Spectacle musical / Concert\",\n",
    "    \"expo-concert\": \"Spectacle musical / Concert\", \"chanson\": \"Spectacle musical / Concert\",\n",
    "\n",
    "    # Danse / Danse-théâtre\n",
    "    \"danse\": \"Danse / Danse-théâtre\", \"danse-théâtre\": \"Danse / Danse-théâtre\",\n",
    "\n",
    "    # Cirque / Clown\n",
    "    \"cirque\": \"Cirque / Clown\", \"clown\": \"Cirque / Clown\",\n",
    "    \"magie\": \"Cirque / Clown\", \"théâtre musical/cirque\": \"Cirque / Clown\",\n",
    "\n",
    "    # Mime / Marionnettes-objets\n",
    "    \"mime\": \"Mime / Marionnettes-objets\",\n",
    "    \"marionnette-objet\": \"Mime / Marionnettes-objets\",\n",
    "    \"marionnette-objet de 7 mois à 4 ans\": \"Mime / Marionnettes-objets\",\n",
    "\n",
    "    # Conte / Poésie / Lecture\n",
    "    \"lecture\": \"Conte / Poésie / Lecture\", \"poésie\": \"Conte / Poésie / Lecture\",\n",
    "    \"conte\": \"Conte / Poésie / Lecture\", \"théâtre-poésie\": \"Conte / Poésie / Lecture\",\n",
    "}\n",
    "\n",
    "df[\"genre_clean\"] = (\n",
    "    df[\"genre\"].str.lower().str.strip()\n",
    "      .str.replace(r\"\\s*/\\s*plein\\s+air\", \"\", regex=True)\n",
    "      .map(genre_map)               # map to group or NaN\n",
    "      .fillna(\"Other\")              # unseen genres -> Other\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e21ff-f3e5-4167-8f4d-63221e9f770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dummies = pd.get_dummies(df[\"genre_clean\"], prefix=\"genre\", drop_first=True)\n",
    "venue_dummies = pd.get_dummies(df[\"festival_theater\"].fillna(\"Inconnu\"),\n",
    "                               prefix=\"venue\", drop_first=True)\n",
    "X = pd.concat([df[[\"n_actors\", \"duration_minutes\"]],\n",
    "               genre_dummies, venue_dummies], axis=1).fillna(0)\n",
    "\n",
    "y = df[\"reprog\"].astype(int)    # 0/1 target\n",
    "\n",
    "print(\"✅ X shape:\", X.shape)\n",
    "print(\"✅ y distribution:\", y.value_counts().to_dict())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e306be-2c70-4cd0-a8cf-c414bfdad54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1e346-c913-465b-a7f2-2bc1f5aae5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621a2f8-bba2-43a3-bbf2-c2871c7775fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e08798-d104-410c-8c3a-f4bde07e231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 0. Imports  (run `pip install xgboost imbalanced-learn` if needed)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import uniform, randint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1. Train / test split\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train class balance:\", Counter(y_train))\n",
    "print(\"Test  class balance:\", Counter(y_test))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2. Pipeline: SMOTETomek → XGBoost\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "pipeline = Pipeline([\n",
    "    (\"smt\", SMOTETomek(sampling_strategy=\"all\", random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=320\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3. Parameter search space  (broad but not huge)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "param_dist = {\n",
    "    \"xgb__n_estimators\"      : randint(150, 500),\n",
    "    \"xgb__max_depth\"         : randint(3, 7),\n",
    "    \"xgb__learning_rate\"     : uniform(0.01, 0.2),\n",
    "    \"xgb__subsample\"         : uniform(0.6, 0.4),\n",
    "    \"xgb__colsample_bytree\"  : uniform(0.6, 0.4),\n",
    "    \"xgb__gamma\"             : uniform(0, 0.5),\n",
    "    \"xgb__reg_alpha\"         : uniform(0, 0.2),\n",
    "    \"xgb__reg_lambda\"        : uniform(0.1, 1.0),\n",
    "    \"xgb__scale_pos_weight\"  : uniform(1.0, 3.0)   # imbalance tweak\n",
    "}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4. RandomizedSearchCV  (30 random combos × 5 folds)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,                 # try 30 random combos\n",
    "    cv=5,                      # 5-fold CV\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5. Run search (may take a few minutes)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n🔎 Best hyper-parameters found:\")\n",
    "for k, v in search.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\n🏆 Best CV balanced-accuracy: {search.best_score_:.3f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6. Evaluate best model on the hold-out test set\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\n📊 Classification report on test set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ba18f-c077-4957-a41f-090f947f6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "THRESH = 0.40                   # ← your chosen cutoff\n",
    "\n",
    "# 1️⃣  Train / test split (same split every run)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2️⃣  Pipeline: SMOTETomek → tuned XGBoost\n",
    "pipe = Pipeline([\n",
    "    (\"smt\", SMOTETomek(random_state=42, sampling_strategy=\"all\")),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.6,\n",
    "        scale_pos_weight=1,          # keep 1.0 since threshold balances recall/precision\n",
    "        eval_metric=\"logloss\",\n",
    "        objective=\"binary:logistic\",\n",
    "        random_state=320\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 3️⃣  Predict with custom threshold\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "y_pred = (proba >= THRESH).astype(int)\n",
    "\n",
    "print(f\"\\n📊 Classification report (threshold = {THRESH}):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 4️⃣  Confusion matrix for visual inspection\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "            yticklabels=[\"True 0\", \"True 1\"])\n",
    "plt.title(f\"Confusion Matrix at Threshold {THRESH}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e5be7-24b8-4219-b786-7fdfba098eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Define range of thresholds\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "f1_scores = []\n",
    "\n",
    "# Compute F1-score for each threshold\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_proba >= t).astype(int)\n",
    "    score = f1_score(y_test, y_pred_t)\n",
    "    f1_scores.append(score)\n",
    "\n",
    "# Get best threshold\n",
    "best_index = np.argmax(f1_scores)\n",
    "best_thresh = thresholds[best_index]\n",
    "best_f1 = f1_scores[best_index]\n",
    "\n",
    "print(f\"🏆 Best threshold for F1-score: {best_thresh:.2f}\")\n",
    "print(f\"📈 Best F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# Recompute predictions and report\n",
    "y_best = (y_proba >= best_thresh).astype(int)\n",
    "print(\"\\n📊 Classification report (best threshold):\")\n",
    "print(classification_report(y_test, y_best))\n",
    "\n",
    "# Optional: Plot F1 vs threshold\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(thresholds, f1_scores, label=\"F1-score\")\n",
    "plt.axvline(best_thresh, color=\"red\", linestyle=\"--\", label=f\"Best: {best_thresh:.2f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"F1-score vs Decision Threshold\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b307f5a-f58b-4212-82ed-6d4de2102e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1. Extract the trained XGBoost model inside the pipeline\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "xgb_model = best_model.named_steps[\"xgb\"]          # adjust key if your step is called \"classifier\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2. Re-apply SMOTETomek so X matches what the model saw\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "X_train_bal, y_train_bal = best_model.named_steps[\"smt\"].fit_resample(X_train, y_train)\n",
    "\n",
    "# It's often enough to sample 5 000 rows for speed\n",
    "sample_idx = np.random.choice(len(X_train_bal), min(5000, len(X_train_bal)), replace=False)\n",
    "X_sample = pd.DataFrame(X_train_bal.iloc[sample_idx], columns=X_train_bal.columns)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3. Build the SHAP TreeExplainer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4. GLOBAL: summary plot (beeswarm)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_sample,\n",
    "    plot_type=\"dot\",       # beeswarm\n",
    "    max_display=20,        # top-20 features\n",
    "    show=True              # pops up in notebook\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5. LOCAL: force plot for a single example (row 0)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "shap.initjs()  # enable JavaScript visualisation in notebook\n",
    "i = 0          # pick any index in X_sample\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values[i],\n",
    "    X_sample.iloc[i],\n",
    "    matplotlib=True        # static render (works inside JupyterLab)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e902c53-588b-429e-9fbe-1f1a3cb49b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdd73c-a8cf-4a2c-98e6-7c419dbb537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de499d92-4aab-4414-b258-42518d152818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3269f-d6fe-4add-ad65-1d08fb84dfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15840774-3a9b-4e06-a5d2-76961a4bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, proba)\n",
    "\n",
    "plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Precision vs Recall for Class 1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b18ca5-e6cb-4ad6-a331-8248a837c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) Copy only needed columns\n",
    "# ----------------------------------------------------------------\n",
    "df = data_draft[[\"reprog\", \"n_actors\", \"genre\"]].copy()\n",
    "df[\"reprog\"] = df[\"reprog\"].astype(int)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) Define the 6 custom genre groups\n",
    "# ----------------------------------------------------------------\n",
    "custom_genre_map = {\n",
    "    \"théâtre\": \"Théâtre / Café-théâtre\",\n",
    "    \"café-théâtre\": \"Théâtre / Café-théâtre\",\n",
    "    \"comédie\": \"Théâtre / Café-théâtre\",\n",
    "    \"drame\": \"Théâtre / Café-théâtre\",\n",
    "    \"boulevard\": \"Théâtre / Café-théâtre\",\n",
    "    \"humour\": \"Théâtre / Café-théâtre\",\n",
    "    \"théâtre sonore\": \"Théâtre / Café-théâtre\",\n",
    "\n",
    "    \"lecture\": \"Conte / Poésie / Lecture\",\n",
    "    \"poésie\": \"Conte / Poésie / Lecture\",\n",
    "    \"conte\": \"Conte / Poésie / Lecture\",\n",
    "    \"théâtre-poésie\": \"Conte / Poésie / Lecture\",\n",
    "\n",
    "    \"danse\": \"Danse / Danse-théâtre\",\n",
    "    \"danse-théâtre\": \"Danse / Danse-théâtre\",\n",
    "\n",
    "    \"cirque\": \"Cirque / Clown\",\n",
    "    \"clown\": \"Cirque / Clown\",\n",
    "    \"théâtre musical/cirque\": \"Cirque / Clown\",\n",
    "    \"magie\": \"Cirque / Clown\",\n",
    "\n",
    "    \"mime\": \"Mime / Marionnettes-objets\",\n",
    "    \"marionnette-objet\": \"Mime / Marionnettes-objets\",\n",
    "    \"marionnette-objet de 7 mois à 4 ans\": \"Mime / Marionnettes-objets\",\n",
    "\n",
    "    \"théâtre musical\": \"Spectacle musical / Concert\",\n",
    "    \"spectacle musical\": \"Spectacle musical / Concert\",\n",
    "    \"concert\": \"Spectacle musical / Concert\",\n",
    "    \"classique\": \"Spectacle musical / Concert\",\n",
    "    \"expo-concert\": \"Spectacle musical / Concert\",\n",
    "    \"chanson\": \"Spectacle musical / Concert\",\n",
    "}\n",
    "\n",
    "# Lowercase + strip to ensure match\n",
    "df[\"genre_lower\"] = df[\"genre\"].str.lower().str.strip()\n",
    "\n",
    "# Remove \"/ plein air\" etc. and normalize formatting\n",
    "df[\"genre_lower\"] = df[\"genre_lower\"].str.replace(r\"\\s*/\\s*plein\\s+air\", \"\", regex=True)\n",
    "\n",
    "# Map to group label\n",
    "df[\"genre_group\"] = df[\"genre_lower\"].replace(custom_genre_map)\n",
    "\n",
    "# Drop any rows with unknown (unmapped) genres\n",
    "df = df[df[\"genre_group\"].notna()]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) One-hot encode grouped genres\n",
    "# ----------------------------------------------------------------\n",
    "genre_dummies = pd.get_dummies(df[\"genre_group\"], prefix=\"genre\", drop_first=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Build final feature matrix and target\n",
    "# ----------------------------------------------------------------\n",
    "X = pd.concat([df[[\"n_actors\"]], genre_dummies], axis=1).fillna(0)\n",
    "y = df[\"reprog\"]\n",
    "\n",
    "# Check\n",
    "print(\"✅ Prepared feature matrix:\")\n",
    "print(\"\\nTarget distribution:\\n\", y.value_counts())\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc338be3-70e6-4581-8e63-535db49e2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎭 Genre group distribution:\")\n",
    "print(df[\"genre_group\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c78835-e68a-4009-bb64-f20db01d664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f326b-886e-496d-b556-533e8da9b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1. Train/test split BEFORE resampling\n",
    "# ----------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2. Apply SMOTE + Tomek links on training set\n",
    "# ----------------------------------------------------------------\n",
    "smt = SMOTETomek(sampling_strategy='all', random_state=320)\n",
    "X_smt, y_smt = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"✅ After SMOTETomek:\")\n",
    "print(pd.Series(y_smt).value_counts())\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3. Train Random Forest\n",
    "# ----------------------------------------------------------------\n",
    "RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,              # limit tree depth\n",
    "    min_samples_leaf=10,      # prevent tiny branches\n",
    "    class_weight='balanced',  # handle imbalance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_smt, y_smt)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4. Evaluate on unmodified test set\n",
    "# ----------------------------------------------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\n📊 Classification report on test set:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe2b79-d3e5-48f0-bc2b-08cebb224e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probas = clf.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probas)\n",
    "\n",
    "plt.plot(thresholds, precision[:-1], label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f166692-37e6-41b0-8fe8-a77b491333ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring=\"f1\")\n",
    "print(\"Mean F1-score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26377fe-159e-4ce9-8485-095a97efe2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=4.5,  # try ratio: n_class_0 / n_class_1\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9d022-85a1-4cdd-911b-264224682c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Optional: Calculate class imbalance ratio for weighting\n",
    "# ----------------------------------------------------------------\n",
    "ratio = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"Class imbalance ratio: {ratio:.2f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Define pipeline: SMOTE + XGBoost\n",
    "# ----------------------------------------------------------------\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('xgb', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=ratio,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5-fold cross-validation (F1 score)\n",
    "# ----------------------------------------------------------------\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1')\n",
    "print(\"\\n✅ Mean F1-score (cross-validated):\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904741a6-a33b-4e2f-ba27-1b372a96add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit full pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on holdout test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719864c9-3791-418b-9fbe-a1c379f06164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Access trained XGBoost model from pipeline\n",
    "xgb_model = pipeline.named_steps[\"xgb\"]\n",
    "\n",
    "# Plot top features\n",
    "plot_importance(xgb_model, max_num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252136b-c496-459a-bfb5-df1f7acd0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_thresh)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (Threshold {:.2f})\".format(threshold))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ecfa8-76ca-46d7-9eba-3e55b39793ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "for weight in [4.5, 3.0, 2.0, 1.0]:\n",
    "    print(f\"\\n🧪 Testing scale_pos_weight = {weight}\")\n",
    "\n",
    "    # Apply SMOTETomek manually (before pipeline) to inspect resampled counts\n",
    "    smt = SMOTETomek(sampling_strategy=\"all\", random_state=42)\n",
    "    X_train_smt, y_train_smt = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Print class distribution before and after\n",
    "    print(\"📊 Before SMOTETomek:\", dict(Counter(y_train)))\n",
    "    print(\"📊 After SMOTETomek: \", dict(Counter(y_train_smt)))\n",
    "\n",
    "    # Now fit pipeline using the same resampled data\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=weight,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train_smt, y_train_smt)\n",
    "\n",
    "    # Evaluate on untouched test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1140423-2b8b-498c-ab48-847a06798330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 0. Imports  (install xgboost / imbalanced-learn if needed)\n",
    "#    !pip install xgboost imbalanced-learn\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1. Train / test split  (stratified 80-20)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Full dataset:\", Counter(y))\n",
    "print(\"Train split :\", Counter(y_train))\n",
    "print(\"Test  split :\", Counter(y_test))\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2. Define SMOTETomek step  (random_state for reproducibility)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "smt = SMOTETomek(sampling_strategy=\"all\", random_state=42)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3. Base XGBoost model  (random_state=320 per your code)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    n_estimators=200,\n",
    "    random_state=320,\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4. Build pipeline  (SMOTETomek ➜ XGBoost)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "pipeline = Pipeline([\n",
    "    (\"smt\", smt),\n",
    "    (\"classifier\", xgb_model)\n",
    "])\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5. Parameter grid for grid-search\n",
    "#    You can add/remove values to make the search finer/coarser.\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "param_grid = {\n",
    "    \"classifier__max_depth\"      : [3, 5, 7],\n",
    "    \"classifier__learning_rate\"  : [0.1, 0.05, 0.01],\n",
    "    \"classifier__subsample\"      : [0.6, 0.8, 1.0],\n",
    "    \"classifier__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"classifier__scale_pos_weight\": [1.0, 2.0, 3.0],   # imbalance control\n",
    "    \"classifier__reg_alpha\"      : [0, 0.01, 0.1],\n",
    "    \"classifier__reg_lambda\"     : [1, 0.1, 0.01],\n",
    "}\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6. GridSearchCV  (5-fold CV, balanced accuracy)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,                  # use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 7. Fit grid search  (this may take a few minutes)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n🔎 Best hyper-parameters:\", grid_search.best_params_)\n",
    "print(\"📈 Best CV balanced accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 8. Inspect class distribution after SMOTETomek in best estimator\n",
    "#    (fit_resample was already run inside CV; we run again for info)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "best_smt = grid_search.best_estimator_.named_steps[\"smt\"]\n",
    "_, y_train_bal = best_smt.fit_resample(X_train, y_train)\n",
    "print(\"\\nAfter SMOTETomek (train set):\", Counter(y_train_bal))\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 9. Evaluate best model on the untouched test set\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\n📊 Classification report on 25% hold-out test set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30366aaa-10e5-4e69-b7cf-13b324c20abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Extract the trained XGBoost model from the pipeline\n",
    "xgb_model = best_model.named_steps[\"classifier\"]\n",
    "\n",
    "# Use TreeExplainer for XGBoost\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Apply SMOTETomek to training data (just for matching the model)\n",
    "X_train_bal, y_train_bal = best_model.named_steps[\"smt\"].fit_resample(X_train, y_train)\n",
    "\n",
    "# Compute SHAP values for a sample of data (can use whole set too)\n",
    "shap_values = explainer.shap_values(X_train_bal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0f4d5-a5b3-4fe2-8add-ca55f2a52f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (global importance of all features)\n",
    "shap.summary_plot(shap_values, X_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535aa78-b3fc-4891-ab58-47d282a20712",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values[0],\n",
    "    X_train_bal.iloc[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86cef0-2d76-4a03-9325-fdce2306f86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983ede1-bc4d-4838-970b-2b2ef39d38e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f75f4-3d24-49ab-a33a-810b4c2ef264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "498f691d-b836-4b9d-a6df-58eb6a661431",
   "metadata": {},
   "source": [
    "### Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1e4f0-9981-41c3-9161-1102b0cfae2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174563c-01fb-43cf-a6d2-bac6b09997c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5a704-ed1c-4ab4-9feb-b52fede7ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "\n",
    "umap_model = UMAP(n_components=2, n_neighbors=3, min_dist=0.1)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    language=\"french\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fba5b-f7e4-4ec3-ab39-8324160a304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def get_top_words(mgp, topic, top_n=5):\n",
    "    word_counts = mgp.cluster_word_distribution[topic]\n",
    "    sorted_words = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [word for word, count in sorted_words[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71d7c0-49f2-4a36-a14e-b0c4f51fcccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "# Sample French docs (very short)\n",
    "docs = [\n",
    "    \"Nouvelle réforme du système éducatif.\",\n",
    "    \"Les manifestations contre la loi ont repris.\",\n",
    "    \"Découverte scientifique majeure en biologie.\",\n",
    "    \"Hausse des prix de l'énergie en Europe.\",\n",
    "    \"Les élections présidentielles approchent.\",\n",
    "]\n",
    "\n",
    "# French-compatible embeddings\n",
    "embedding_model = SentenceTransformer(\"Lajavaness/sentence-camembert-large\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "# Fix UMAP for small dataset\n",
    "umap_model = UMAP(n_neighbors=2, n_components=2, min_dist=0.1, metric=\"cosine\")\n",
    "\n",
    "# Fix HDBSCAN for small dataset\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, prediction_data=True)\n",
    "\n",
    "# BERTopic with custom UMAP and HDBSCAN\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    language=\"french\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# Display topics\n",
    "print(\"\\n📌 Topic Summary:\")\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "print(\"\\n🧠 Top words per topic:\")\n",
    "for topic_id in topic_model.get_topic_freq().Topic:\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    print(f\"\\nTopic {topic_id}:\")\n",
    "    for word, weight in topic_model.get_topic(topic_id):\n",
    "        print(f\"  {word:<15} {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f0459-d5c1-4463-9c86-39d34d3f4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load French stopwords\n",
    "french_stopwords = set(stopwords.words(\"french\"))\n",
    "\n",
    "# Extract and print labels\n",
    "print(\"\\n🏷️ Clean topic labels (top 2 non-stopwords):\")\n",
    "for topic_id in topic_model.get_topic_freq().Topic:\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    top_words = topic_model.get_topic(topic_id)\n",
    "    \n",
    "    # Filter out stopwords and empty strings\n",
    "    keywords = [word for word, _ in top_words if word.lower() not in french_stopwords and word.strip() != \"\"]\n",
    "    \n",
    "    # Take the first two meaningful words\n",
    "    label = \" / \".join(keywords[:2])\n",
    "    print(f\"Topic {topic_id}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9c65d-3719-4dd4-b912-0474428d49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords once\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "FRENCH_STOPWORDS = set(stopwords.words(\"french\"))\n",
    "\n",
    "def build_topic_model(docs, n_neighbors=2, min_cluster_size=2):\n",
    "    embedding_model = SentenceTransformer(\"Lajavaness/sentence-camembert-large\")\n",
    "    embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=2, min_dist=0.1, metric=\"cosine\")\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=1, prediction_data=True)\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        language=\"french\",\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    topic_model.fit(docs, embeddings)\n",
    "    return topic_model\n",
    "\n",
    "def get_clean_topic_labels(topic_model, top_n=2, stopwords_set=FRENCH_STOPWORDS):\n",
    "    labels = {}\n",
    "    for topic_id in topic_model.get_topic_freq().Topic:\n",
    "        if topic_id == -1:\n",
    "            continue\n",
    "        keywords = [\n",
    "            w for w, _ in topic_model.get_topic(topic_id)\n",
    "            if w.lower() not in stopwords_set and w.strip()\n",
    "        ]\n",
    "        labels[topic_id] = \" / \".join(keywords[:top_n])\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadc36b-1fd3-492a-bf0a-eca8f8424499",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft.loc[10,\"synopsis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce7dd4-cbf3-443d-9c61-c3a20f2ca0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data_draft.loc[11, \"synopsis\"]\n",
    "docs = [s.strip() for s in text.split(\".\") if len(s.strip().split()) > 4]\n",
    "\n",
    "topic_model = build_topic_model(docs)\n",
    "topic_labels = get_clean_topic_labels(topic_model)\n",
    "\n",
    "print(docs)\n",
    "for topic_id, label in topic_labels.items():\n",
    "    print(f\"Topic {topic_id}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f612d-4cfa-4aeb-a778-4761bdab1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "text = data_draft.loc[10, \"synopsis\"]\n",
    "keywords = kw_model.extract_keywords(text, top_n=5)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e1822-56ac-4bec-add2-c627631d44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "text = data_draft.loc[100, \"synopsis\"]\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", n=1, top=5)\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw, score in keywords:\n",
    "    print(f\"{kw}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178811a7-2e8e-43df-8232-fef52deffca0",
   "metadata": {},
   "source": [
    "### Structure production info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba15dc-4d8a-4f4a-af76-12e4924b4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_production_metadata_gpt(input_string, api_key):\n",
    "\n",
    "    prompt = build_production_prompt(input_string)\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You extract structured metadata from production descriptions.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=1500,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9aa42f-7a7f-4e87-b006-fdce665c712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_production_prompt(production_text):\n",
    "    return (\n",
    "        \"You will receive a string in French that may contain names of theaters and other cultural institutions that have participated in the making of a theater show.\"\n",
    "        \"There are three types of participation: 'production/corpoduction', 'funding' and 'other'.\"\n",
    "        \"Your task is to extract these data in JSON format:\\n\\n\"\n",
    "        \"{\\n\"\n",
    "        ' \"coproduction\": [...],'\n",
    "        ' \"funding\": [...], '\n",
    "        ' \"other\":[...] '\n",
    "        ' \"main\":[...] \\n '\n",
    "        \"}\"\n",
    "        \"If it is 'production/coproduction', this will be indicated by key words such as 'produit', 'copoduction', 'coréalisation', etc. Return only the names of the institutions separated by ',' \"\n",
    "        \"If it is 'funding', this will be indicated by key words such as 'financement', 'soutiens', 'avec le soutien de', 'avec l'aide de', etc. Return only the names of the institutions separated by ',' \"\n",
    "        \"If you are not certain about where an institution should be put, add it 'other'. All institutions should be placed in a category. Keep the full name of each institutions (inclduing countries or other indicators)\" \n",
    "        \"If you detect is a press institution, ignore it. Ignore any names of people.\"\n",
    "        \"If you can clearly identify the institution that created the show add it in 'main' \"\n",
    "        \"Do not return any explanation — only valid JSON.\\n\\n\"\n",
    "        f\"Input:\\n{production_text}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1498926-f4b0-40c9-86a1-db31fad31403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_production_gpt_extraction(df, api_key, col=\"production_info\", new_col=\"API_production\", max_rows=None):\n",
    "    \n",
    "    subset = df if max_rows is None else df.iloc[:max_rows]\n",
    "    results = []\n",
    "\n",
    "    for i, row in subset.iterrows():\n",
    "        txt = row[col]\n",
    "        if isinstance(txt, str) and txt.strip():\n",
    "            print(f\"Processing row {i}…\")\n",
    "            resp = extract_production_metadata_gpt(txt, api_key)\n",
    "            time.sleep(0.3)\n",
    "        else:\n",
    "            resp = None\n",
    "        results.append(resp)\n",
    "\n",
    "    # now assign in one shot—no chained assignment\n",
    "    df[new_col] = pd.Series(results, index=subset.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d94e1a-e84a-445b-aa7e-b8d14c33ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft = run_production_gpt_extraction(data_draft,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973f462-1571-4fbb-9da8-a23796099df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft.loc[:, \"API_production\"] = data_draft[\"API_production\"].apply(clean_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d366723-3aee-4840-a32b-3244bc8d85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_draft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
